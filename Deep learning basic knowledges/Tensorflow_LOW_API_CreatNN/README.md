# åœ¨ä¸ä½¿ç”¨é«˜çº§API kerasçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è‡ªå®šä¹‰å‡½æ•°æ„å»ºç¥ç»ç½‘ç»œ.

Use tf.Variable to modify the state of a variable
Explain the difference between a variable and a constant
Train a Neural Network on a TensorFlow dataset
Programming frameworks like TensorFlow not only cut down on time spent coding, but can also perform optimizations that speed up the code itself.

## Table of Contents
- [1- Packages](#1)
    - [1.1 - Checking TensorFlow Version](#1-1)
- [2 - Basic Optimization with GradientTape](#2)
    - [2.1 - Linear Function](#2-1)
        - [Exercise 1 - linear_function](#ex-1)
    - [2.2 - Computing the Sigmoid](#2-2)
        - [Exercise 2 - sigmoid](#ex-2)
    - [2.3 - Using One Hot Encodings](#2-3)
        - [Exercise 3 - one_hot_matrix](#ex-3)
    - [2.4 - Initialize the Parameters](#2-4)
        - [Exercise 4 - initialize_parameters](#ex-4)
- [3 - Building Your First Neural Network in TensorFlow](#3)
    - [3.1 - Implement Forward Propagation](#3-1)
        - [Exercise 5 - forward_propagation](#ex-5)
    - [3.2 Compute the Total Loss](#3-2)
        - [Exercise 6 - compute_total_loss](#ex-6)
    - [3.3 - Train the Model](#3-3)
- [4 - Bibliography](#4)

<a name='1'></a>
## 1 - Packages

        import h5py
        import numpy as np
        import tensorflow as tf
        import matplotlib.pyplot as plt
        from tensorflow.python.framework.ops import EagerTensor
        from tensorflow.python.ops.resource_variable_ops import ResourceVariable
        import time

<a name='1-1'></a>
### 1.1 - Checking TensorFlow Version 

You will be using v2.3 for this assignment, for maximum speed and efficiency.
![1](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/053ca842-502b-4262-a3af-e74158f2fe14)
<a name='2'></a>
## 2 - Basic Optimization with GradientTape

The beauty of TensorFlow 2 is in its simplicity. Basically, all you need to do is implement forward propagation through a computational graph. TensorFlow will compute the derivatives for you, by moving backwards through the graph recorded with `GradientTape`. All that's left for you to do then is specify the cost function and optimizer you want to use! 

When writing a TensorFlow program, the main object to get used and transformed is the `tf.Tensor`. These tensors are the TensorFlow equivalent of Numpy arrays, i.e. multidimensional arrays of a given data type that also contain information about the computational graph.

Below, you'll use `tf.Variable` to store the state of your variables. Variables can only be created once as its initial value defines the variable shape and type. Additionally, the `dtype` arg in `tf.Variable` can be set to allow data to be converted to that type. But if none is specified, either the datatype will be kept if the initial value is a Tensor, or `convert_to_tensor` will decide. It's generally best for you to specify directly, so nothing breaks!
<a name='2'></a>
## 2 - æ¢¯åº¦å¸¦çš„åŸºæœ¬ä¼˜åŒ–

TensorFlow 2çš„é­…åŠ›åœ¨äºå…¶ç®€å•æ€§ã€‚åŸºæœ¬ä¸Šï¼Œæ‚¨éœ€è¦åšçš„å°±æ˜¯é€šè¿‡è®¡ç®—å›¾å®ç°å‰å‘ä¼ æ’­ã€‚TensorFlowå°†é€šè¿‡ "GradientTape "è®°å½•çš„å›¾å‘åç§»åŠ¨ï¼Œä¸ºæ‚¨è®¡ç®—å¯¼æ•°ã€‚æ‚¨æ‰€è¦åšçš„å°±æ˜¯æŒ‡å®šæ‚¨è¦ä½¿ç”¨çš„ä»£ä»·å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼

åœ¨ç¼–å†™TensorFlowç¨‹åºæ—¶ï¼Œä½¿ç”¨å’Œè½¬æ¢çš„ä¸»è¦å¯¹è±¡æ˜¯`tf.Tensor`ã€‚è¿™äº›å¼ é‡ç›¸å½“äºTensorFlowçš„Numpyæ•°ç»„ï¼Œå³ç»™å®šæ•°æ®ç±»å‹çš„å¤šç»´æ•°ç»„ï¼ŒåŒæ—¶åŒ…å«è®¡ç®—å›¾çš„ä¿¡æ¯ã€‚

ä¸‹é¢ï¼Œæ‚¨å°†ä½¿ç”¨`tf.Variable`æ¥å­˜å‚¨å˜é‡çš„çŠ¶æ€ã€‚å˜é‡åªèƒ½åˆ›å»ºä¸€æ¬¡ï¼Œå› ä¸ºå®ƒçš„åˆå§‹å€¼å®šä¹‰äº†å˜é‡çš„å½¢çŠ¶å’Œç±»å‹ã€‚æ­¤å¤–ï¼Œå¯ä»¥è®¾ç½®`tf.Variable`ä¸­çš„`dtype`å‚æ•°ï¼Œä»¥ä¾¿å°†æ•°æ®è½¬æ¢ä¸ºè¯¥ç±»å‹ã€‚ä½†å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œå¦‚æœåˆå§‹å€¼æ˜¯å¼ é‡ï¼Œæ•°æ®ç±»å‹å°†è¢«ä¿ç•™ï¼Œæˆ–è€…ç”±`convert_to_tensor`å†³å®šã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæœ€å¥½æ˜¯ç›´æ¥æŒ‡å®šï¼Œè¿™æ ·å°±ä¸ä¼šå‡ºé”™ï¼


Here you'll call the TensorFlow dataset created on a HDF5 file, which you can use in place of a Numpy array to store your datasets. You can think of this as a TensorFlow data generator! 

You will use the Hand sign data set, that is composed of images with shape 64x64x3.

è¿™é‡Œä½ å°†è°ƒç”¨åœ¨HDF5æ–‡ä»¶ä¸Šåˆ›å»ºçš„TensorFlowæ•°æ®é›†ï¼Œä½ å¯ä»¥ç”¨å®ƒæ¥ä»£æ›¿Numpyæ•°ç»„æ¥å­˜å‚¨ä½ çš„æ•°æ®é›†ã€‚æ‚¨å¯ä»¥å°†å…¶è§†ä¸ºTensorFlowæ•°æ®ç”Ÿæˆå™¨ï¼

æ‚¨å°†ä½¿ç”¨æ‰‹åŠ¿æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±å½¢çŠ¶ä¸º64x64x3çš„å›¾åƒç»„æˆã€‚

## åŠ è½½æ•°æ®ï¼Œå¹¶æŠŠæ•°æ®è½¬æ¢æˆTensorflowéœ€è¦çš„æ ·å­

è¿™æ®µä»£ç ä½¿ç”¨äº†h5pyåº“æ¥è¯»å–HDF5æ–‡ä»¶æ ¼å¼ä¸­çš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚

h5py.File('datasets/train_signs.h5', "r")ï¼šè¿™è¡Œä»£ç æ‰“å¼€åä¸ºtrain_signs.h5çš„HDF5æ–‡ä»¶ï¼Œå¹¶ä»¥åªè¯»æ¨¡å¼æ‰“å¼€å®ƒã€‚HDF5æ˜¯ä¸€ç§ç”¨äºå­˜å‚¨å’Œç»„ç»‡å¤§å‹æ•°æ®é›†çš„æ–‡ä»¶æ ¼å¼ï¼Œé€šå¸¸åœ¨æœºå™¨å­¦ä¹ ä¸­ç”¨äºå­˜å‚¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚

h5py.File('datasets/test_signs.h5', "r")ï¼šè¿™è¡Œä»£ç æ‰“å¼€åä¸ºtest_signs.h5çš„HDF5æ–‡ä»¶ï¼Œå¹¶ä»¥åªè¯»æ¨¡å¼æ‰“å¼€å®ƒï¼Œç”¨äºå­˜å‚¨æµ‹è¯•æ•°æ®ã€‚

ä¸€æ—¦è¿™ä¸¤ä¸ªHDF5æ–‡ä»¶è¢«æ‰“å¼€ï¼Œä½ å°±å¯ä»¥ä½¿ç”¨train_datasetå’Œtest_datasetä¸¤ä¸ªå˜é‡æ¥è®¿é—®å…¶ä¸­çš„æ•°æ®é›†å’Œç›¸å…³ä¿¡æ¯ã€‚é€šå¸¸ï¼Œè¿™äº›æ–‡ä»¶ä¼šåŒ…å«è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„ç‰¹å¾ï¼ˆä¾‹å¦‚å›¾åƒæ•°æ®ï¼‰ä»¥åŠå¯¹åº”çš„æ ‡ç­¾ï¼ˆä¾‹å¦‚å›¾åƒæ‰€å±çš„ç±»åˆ«ï¼‰ã€‚

è¦è¿›ä¸€æ­¥ä½¿ç”¨è¿™äº›æ•°æ®é›†ï¼Œä½ å¯ä»¥é€šè¿‡h5pyåº“ä¸­çš„æ–¹æ³•æ¥è·å–å…¶ä¸­çš„æ•°æ®å’Œå…ƒæ•°æ®ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ç±»ä¼¼train_dataset['features']çš„æ–¹å¼æ¥è·å–è®­ç»ƒæ•°æ®é›†ä¸­çš„ç‰¹å¾æ•°æ®ï¼Œtrain_dataset['labels']æ¥è·å–è®­ç»ƒæ•°æ®é›†ä¸­çš„æ ‡ç­¾æ•°æ®ï¼Œç­‰ç­‰ã€‚
```
train_dataset = h5py.File('datasets/train_signs.h5', "r")
test_dataset = h5py.File('datasets/test_signs.h5', "r")
```
**è¿›è¡Œæå–å’Œè½¬æ¢**

è¿™æ®µä»£ç ä½¿ç”¨TensorFlowä¸­çš„tf.data.Dataset.from_tensor_slicesæ–¹æ³•ä»numpyæ•°ç»„ï¼ˆæˆ–å¼ é‡ï¼‰ä¸­åˆ›å»ºäº†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†çš„tf.data.Datasetå¯¹è±¡ã€‚

train_dataset['train_set_x']ï¼šè¿™éƒ¨åˆ†ä»£ç ä»train_datasetå¯¹è±¡ä¸­è·å–äº†åä¸ºtrain_set_xçš„æ•°æ®ï¼Œè¿™å¯èƒ½æ˜¯è®­ç»ƒæ•°æ®é›†çš„ç‰¹å¾æ•°æ®ï¼ˆä¾‹å¦‚å›¾åƒæ•°æ®ï¼‰ã€‚

train_dataset['train_set_y']ï¼šè¿™éƒ¨åˆ†ä»£ç ä»train_datasetå¯¹è±¡ä¸­è·å–äº†åä¸ºtrain_set_yçš„æ•°æ®ï¼Œè¿™å¯èƒ½æ˜¯è®­ç»ƒæ•°æ®é›†çš„æ ‡ç­¾æ•°æ®ï¼ˆä¾‹å¦‚å›¾åƒç±»åˆ«æ ‡ç­¾ï¼‰ã€‚

test_dataset['test_set_x']ï¼šè¿™éƒ¨åˆ†ä»£ç ä»test_datasetå¯¹è±¡ä¸­è·å–äº†åä¸ºtest_set_xçš„æ•°æ®ï¼Œè¿™å¯èƒ½æ˜¯æµ‹è¯•æ•°æ®é›†çš„ç‰¹å¾æ•°æ®ã€‚

test_dataset['test_set_y']ï¼šè¿™éƒ¨åˆ†ä»£ç ä»test_datasetå¯¹è±¡ä¸­è·å–äº†åä¸ºtest_set_yçš„æ•°æ®ï¼Œè¿™å¯èƒ½æ˜¯æµ‹è¯•æ•°æ®é›†çš„æ ‡ç­¾æ•°æ®ã€‚
```python
x_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_x'])
y_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_y'])

x_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_x'])
y_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_y'])
```

Since TensorFlow Datasets are generators, you can't access directly the contents unless you iterate over them in a for loop, or by explicitly creating a Python iterator using `iter` and consuming its
elements using `next`. Also, you can inspect the `shape` and `dtype` of each element using the `element_spec` attribute.

ç”±äºTensorFlowæ•°æ®é›†æ˜¯ç”Ÿæˆå™¨ï¼Œä½ ä¸èƒ½ç›´æ¥è®¿é—®å…¶å†…å®¹ï¼Œé™¤éä½ åœ¨ä¸€ä¸ªforå¾ªç¯ä¸­éå†å®ƒä»¬ï¼Œæˆ–è€…é€šè¿‡ä½¿ç”¨`iter`æ˜¾å¼åœ°åˆ›å»ºä¸€ä¸ªPythonè¿­ä»£å™¨ï¼Œå¹¶ä½¿ç”¨`next`æ¶ˆè€—å®ƒçš„
å…ƒç´ ã€‚å¦å¤–ï¼Œä½ å¯ä»¥ä½¿ç”¨`element_spec`å±æ€§æ£€æŸ¥æ¯ä¸ªå…ƒç´ çš„`shape`å’Œ`dtype`ã€‚
![2](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/479da84a-4625-4a23-b241-30955be6ef2c)


The dataset that you'll be using during this assignment is a subset of the sign language digits. It contains six different classes representing the digits from 0 to 5.

æœ¬ä½œä¸šä¸­æ‚¨å°†ä½¿ç”¨çš„æ•°æ®é›†æ˜¯æ‰‹è¯­æ•°å­—çš„å­é›†ã€‚å®ƒåŒ…å«å…­ä¸ªä¸åŒçš„ç±»åˆ«ï¼Œä»£è¡¨ä»0åˆ°5çš„æ•°å­—ã€‚


è¿™æ®µä»£ç é€šè¿‡éå†TensorFlow Datasetä¸­çš„y_trainï¼ˆå‡è®¾y_trainæ˜¯ä¸€ä¸ªTensorFlow Datasetå¯¹è±¡ï¼‰æ¥è·å–è®­ç»ƒæ•°æ®é›†ä¸­çš„å”¯ä¸€æ ‡ç­¾ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ä¸€ä¸ªé›†åˆï¼ˆunique_labelsï¼‰ä¸­ã€‚

unique_labels = set()ï¼šè¿™ä¸€è¡Œåˆ›å»ºäº†ä¸€ä¸ªç©ºçš„é›†åˆï¼Œç”¨äºå­˜å‚¨å”¯ä¸€çš„æ ‡ç­¾ã€‚

for element in y_train:ï¼šè¿™ä¸ªå¾ªç¯éå†äº†y_trainä¸­çš„æ¯ä¸ªå…ƒç´ ã€‚

unique_labels.add(element.numpy())ï¼šåœ¨å¾ªç¯ä¸­ï¼Œä»£ç ä½¿ç”¨addæ–¹æ³•å°†æ¯ä¸ªå…ƒç´ ï¼ˆå‡è®¾æ˜¯ä¸€ä¸ªTensorï¼‰çš„numpyè¡¨ç¤ºï¼ˆå³å…ƒç´ çš„å®é™…å€¼ï¼‰æ·»åŠ åˆ°unique_labelsé›†åˆä¸­ã€‚è¿™æ ·ï¼Œé›†åˆunique_labelså°±ä¼šåŒ…å«è®­ç»ƒæ•°æ®é›†ä¸­çš„æ‰€æœ‰å”¯ä¸€æ ‡ç­¾å€¼ã€‚

print(unique_labels)ï¼šæœ€åï¼Œä»£ç æ‰“å°è¾“å‡ºäº†unique_labelsé›†åˆï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒæ•°æ®é›†ä¸­çš„æ‰€æœ‰å”¯ä¸€æ ‡ç­¾å€¼ã€‚

æ€»ä¹‹ï¼Œè¿™æ®µä»£ç çš„ä½œç”¨æ˜¯è·å–è®­ç»ƒæ•°æ®é›†y_trainä¸­çš„æ‰€æœ‰å”¯ä¸€æ ‡ç­¾ï¼Œå¹¶å°†å…¶æ‰“å°è¾“å‡ºã€‚
```python
unique_labels = set()
for element in y_train:
    unique_labels.add(element.numpy())
print(unique_labels)
```

![1](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/c9e673dd-75a3-48ba-8d8d-d4a8403a7a71)

You can see some of the images in the dataset by running the following cell.

æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å•å…ƒæ ¼æŸ¥çœ‹æ•°æ®é›†ä¸­çš„éƒ¨åˆ†å›¾åƒã€‚

è¿™æ®µä»£ç ä½¿ç”¨matplotlibåº“åœ¨ä¸€ä¸ª5x5çš„å›¾åƒç½‘æ ¼ä¸­æ˜¾ç¤ºè®­ç»ƒæ•°æ®é›†ä¸­çš„å›¾åƒå’Œå¯¹åº”çš„æ ‡ç­¾ã€‚

images_iter = iter(x_train)ï¼šè¿™è¡Œä»£ç å°†è®­ç»ƒæ•°æ®é›†x_trainè½¬æ¢ä¸ºä¸€ä¸ªè¿­ä»£å™¨å¯¹è±¡images_iterã€‚è¿­ä»£å™¨å¯ä»¥ç”¨äºé€ä¸ªè®¿é—®æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚

labels_iter = iter(y_train)ï¼šè¿™è¡Œä»£ç å°†è®­ç»ƒæ•°æ®é›†y_trainè½¬æ¢ä¸ºä¸€ä¸ªè¿­ä»£å™¨å¯¹è±¡labels_iterã€‚åŒæ ·ï¼Œè¿­ä»£å™¨å¯ä»¥ç”¨äºé€ä¸ªè®¿é—®æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚

plt.figure(figsize=(10, 10))ï¼šè¿™è¡Œä»£ç åˆ›å»ºä¸€ä¸ªå¤§å°ä¸º10x10çš„æ–°å›¾åƒçª—å£ã€‚

for i in range(25):ï¼šè¿™ä¸ªå¾ªç¯éå†25æ¬¡ï¼Œå³åœ¨å›¾åƒç½‘æ ¼ä¸­æ˜¾ç¤º25å¼ å›¾åƒã€‚

ax = plt.subplot(5, 5, i + 1)ï¼šè¿™è¡Œä»£ç åˆ›å»ºä¸€ä¸ªå­å›¾ï¼Œå°†å½“å‰å›¾åƒæ”¾åœ¨5x5çš„ç½‘æ ¼ä¸­çš„ç¬¬i+1ä¸ªä½ç½®ã€‚

plt.imshow(next(images_iter).numpy().astype("uint8"))ï¼šè¿™è¡Œä»£ç ä½¿ç”¨next(images_iter)è·å–è®­ç»ƒæ•°æ®é›†ä¸­çš„ä¸‹ä¸€ä¸ªå›¾åƒï¼Œå¹¶ä½¿ç”¨imshowæ–¹æ³•æ˜¾ç¤ºå›¾åƒã€‚.numpy()æ–¹æ³•å°†TensorFlowå¼ é‡è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œ.astype("uint8")å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ— ç¬¦å·8ä½æ•´æ•°ã€‚

plt.title(next(labels_iter).numpy().astype("uint8"))ï¼šè¿™è¡Œä»£ç ä½¿ç”¨next(labels_iter)è·å–è®­ç»ƒæ•°æ®é›†ä¸­ä¸‹ä¸€ä¸ªå›¾åƒå¯¹åº”çš„æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨titleæ–¹æ³•å°†å…¶æ˜¾ç¤ºä¸ºå›¾åƒçš„æ ‡é¢˜ã€‚åŒæ ·ï¼Œ.numpy()æ–¹æ³•å°†TensorFlowå¼ é‡è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œ.astype("uint8")å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ— ç¬¦å·8ä½æ•´æ•°ã€‚

plt.axis("off")ï¼šè¿™è¡Œä»£ç å…³é—­å›¾åƒçš„åæ ‡è½´æ˜¾ç¤ºï¼Œä»¥ä¾¿æ›´å¥½åœ°æŸ¥çœ‹å›¾åƒæœ¬èº«ã€‚

```python
images_iter = iter(x_train)
labels_iter = iter(y_train)
plt.figure(figsize=(10, 10))
for i in range(25):
    ax = plt.subplot(5, 5, i + 1)
    plt.imshow(next(images_iter).numpy().astype("uint8"))
    plt.title(next(labels_iter).numpy().astype("uint8"))
    plt.axis("off")
```
![5](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/e6cb31aa-333e-49da-9915-161ef115448e)

TensorFlowæ•°æ®é›†å’ŒNumpyæ•°ç»„ä¹‹é—´è¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„åŒºåˆ«ï¼š å¦‚æœæ‚¨éœ€è¦è½¬æ¢æ•°æ®é›†ï¼Œæ‚¨éœ€è¦è°ƒç”¨`map`æ–¹æ³•ï¼Œå°†ä½œä¸ºå‚æ•°ä¼ é€’çš„å‡½æ•°åº”ç”¨åˆ°æ¯ä¸ªå…ƒç´ ä¸Šã€‚

## Normalization the images 

å’Œåœ¨Numpyä¸­çš„å¸¸è§„æ“ä½œç›¸åŒï¼Œåƒç´ çš„æœ€å¤§å€¼çš„255ï¼Œæˆ‘ä»¬è®©æ¯ä¸€ä¸ªdim_layerä¸­çš„å…ƒç´ éƒ½é™¤ä»¥255ï¼Œå½’ä¸€åŒ–åˆ°[0,1]çš„èŒƒå›´ä¸­ï¼Œç„¶åå°†3ç»´arrayï¼Œ
è½¬æ¢æˆ1ç»´arrayï¼ˆ64*64*3ï¼‰.

è€Œåœ¨TensorFlow çš„ä¸Šé¢ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æŠŠå°†å›¾åƒåƒç´ å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»å‹ã€‚å†å°†åƒç´ å€¼é™¤ä»¥255ï¼Œå°†å…¶å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´å†…ã€‚

è€Œä»£ç å’ŒNumpyä¸­æ—¶ï¼Œæœ‰æ‰€ä¸åŒ.
è¿™æ˜¯Numpyä¸­ï¼š image = image.astype(np.float32) / 255.0
è¿™æ˜¯Tfä¸­ï¼š    image = tf.cast(image, tf.float32) / 255.0

ç„¶åå°†è¿™ä¸ª3ç»´æ•°ç»„è½¬æˆ1ç»´ï¼š64*64*3
ä½¿ç”¨reshapeï¼ˆï¼‰å‡½æ•°è¾¾åˆ°æ•ˆæœ    image = tf.reshape(image, [-1,])
```python
def normalize(image):
    """
    Transform an image into a tensor of shape (64 * 64 * 3, )
    and normalize its components.
    
    Arguments
    image - Tensor.
    
    Returns: 
    result -- Transformed tensor 
    """
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.reshape(image, [-1,])
    return image
```
ç„¶åè¿™æ®µä»£ç ä½¿ç”¨äº†TensorFlowä¸­çš„mapæ–¹æ³•å¯¹è®­ç»ƒæ•°æ®é›†x_trainå’Œæµ‹è¯•æ•°æ®é›†x_testä¸­çš„æ¯ä¸ªå›¾åƒåº”ç”¨normalizeå‡½æ•°è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚

x_trainå’Œx_testæ˜¯TensorFlowçš„tf.data.Datasetå¯¹è±¡ï¼Œè¡¨ç¤ºè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ã€‚

normalizeæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ˜¯å°†å›¾åƒè¿›è¡Œå½’ä¸€åŒ–çš„å‡½æ•°ï¼Œå°†åƒç´ å€¼é™¤ä»¥255æ¥å°†åƒç´ å€¼ç¼©æ”¾åˆ°[0, 1]çš„èŒƒå›´å†…ã€‚

new_train = x_train.map(normalize)ï¼šè¿™è¡Œä»£ç ä½¿ç”¨mapæ–¹æ³•ï¼Œå°†normalizeå‡½æ•°åº”ç”¨äºx_trainæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒã€‚è¿™æ ·ï¼Œè®­ç»ƒæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒéƒ½ä¼šè¢«å½’ä¸€åŒ–å¤„ç†ï¼Œå¹¶å­˜å‚¨åœ¨new_trainæ•°æ®é›†ä¸­ã€‚

new_test = x_test.map(normalize)ï¼šè¿™è¡Œä»£ç ä½¿ç”¨mapæ–¹æ³•ï¼Œå°†normalizeå‡½æ•°åº”ç”¨äºx_testæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒã€‚è¿™æ ·ï¼Œæµ‹è¯•æ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒä¹Ÿä¼šè¢«å½’ä¸€åŒ–å¤„ç†ï¼Œå¹¶å­˜å‚¨åœ¨new_testæ•°æ®é›†ä¸­ã€‚

æœ€ç»ˆï¼Œnew_trainå’Œnew_testæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒéƒ½è¢«å½’ä¸€åŒ–å¤„ç†ï¼Œä»¥ä¾¿åç»­çš„å›¾åƒå¤„ç†å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€‚è¿™æ˜¯åˆ©ç”¨tf.data.Dataset.map()æ–¹æ³•å¯¹æ•°æ®é›†ä¸­çš„å…ƒç´ è¿›è¡Œé¢„å¤„ç†çš„å¸¸è§ç”¨æ³•ã€‚
```
new_train = x_train.map(normalize)
new_test = x_test.map(normalize)
```
ä½¿ç”¨element_specåç¼€æ¥æŸ¥è¯¢æ–°å¤åˆ¶çš„è®­ç»ƒæ•°ç»„çš„å†…å®¹
```
new_train.element_spec
```
![1](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/759f930a-fb1c-413b-b177-7be48855ebde)
![2](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/56a1e4fe-812c-40ff-9ca2-2cf99e14cbc7)

<a name='2-1'></a>
### 2.1 - Linear Function

Let's begin this programming exercise by computing the following equation: $Y = WX + b$, where $W$ and $X$ are random matrices and b is a random vector. 

è®©æˆ‘ä»¬ä»è®¡ç®—ä¸‹åˆ—æ–¹ç¨‹å¼€å§‹è¿™ä¸ªç¼–ç¨‹ç»ƒä¹ ï¼šğ‘Œ=ğ‘‹+ğ‘ ï¼Œå…¶ä¸­ ğ‘‹ å’Œ ğ‘‹ æ˜¯éšæœºçŸ©é˜µï¼Œb æ˜¯éšæœºå‘é‡ã€‚

<a name='ex-1'></a>
### Exercise 1 - linear_function

Compute $WX + b$ where $W, X$, and $b$ are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1). As an example, this is how to define a constant X with the shape (3,1):
```python
X = tf.constant(np.random.randn(3,1), name = "X")

```
Note that the difference between `tf.constant` and `tf.Variable` is that you can modify the state of a `tf.Variable` but cannot change the state of a `tf.constant`.

You might find the following functions helpful: 
- tf.matmul(..., ...) to do a matrix multiplication
- tf.add(..., ...) to do an addition
- np.random.randn(...) to initialize randomly

 ç»ƒä¹  1 - çº¿æ€§å‡½æ•°
è®¡ç®— ğ‘‹+ğ‘ï¼Œå…¶ä¸­ğ‘‹, ğ‘å’Œ ğ‘ä»éšæœºæ­£æ€åˆ†å¸ƒä¸­æŠ½å–ã€‚W çš„å½¢çŠ¶ä¸º (4,3)ï¼ŒX ä¸º (3,1)ï¼Œb ä¸º (4,1)ã€‚ä¸¾ä¾‹è¯´æ˜ï¼Œå¦‚ä½•å®šä¹‰å½¢çŠ¶ä¸º(3,1)çš„å¸¸æ•°Xï¼š

X = tf.constant(np.random.randn(3,1), name = "X")
è¯·æ³¨æ„ï¼Œtf.constantå’Œtf.Variableçš„åŒºåˆ«åœ¨äºï¼Œæ‚¨å¯ä»¥ä¿®æ”¹tf.Variableçš„çŠ¶æ€ï¼Œä½†ä¸èƒ½æ”¹å˜tf.constantçš„çŠ¶æ€ã€‚

æ‚¨å¯èƒ½ä¼šå‘ç°ä»¥ä¸‹å‡½æ•°å¾ˆæœ‰ç”¨ï¼š

tf.matmul(...,...)è¿›è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—
tf.add(...,...)è¿›è¡ŒåŠ æ³•è¿ç®—
np.random.randn(...)ç”¨äºéšæœºåˆå§‹åŒ–


è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œæ ¹æ®ç»™å®šçš„åˆå§‹åŒ–è§„åˆ™åˆ›å»ºäº†éšæœºå¼ é‡ï¼Œå¹¶è®¡ç®—å‡ºçº¿æ€§å‡½æ•°çš„è¾“å‡ºã€‚

np.random.seed(1)ï¼šè¿™è¡Œä»£ç è®¾ç½®äº†éšæœºç§å­ï¼Œä»¥ç¡®ä¿éšæœºæ•°çš„ç”Ÿæˆä¸é¢„æœŸç»“æœä¸€è‡´ã€‚

X = tf.constant(np.random.randn(3,1), name = "X")ï¼šè¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªåä¸ºXçš„å¸¸é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º(3, 1)ï¼Œå€¼ä¸ºéšæœºç”Ÿæˆçš„æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ•°å­—ã€‚

W = tf.constant(np.random.randn(4,3), name = "W")ï¼šè¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªåä¸ºWçš„å¸¸é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º(4, 3)ï¼Œå€¼ä¸ºéšæœºç”Ÿæˆçš„æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ•°å­—ã€‚

b = tf.constant(np.random.randn(4,1), name = "b")ï¼šè¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªåä¸ºbçš„å¸¸é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º(4, 1)ï¼Œå€¼ä¸ºéšæœºç”Ÿæˆçš„æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ•°å­—ã€‚

Y = tf.matmul(W, X) + bï¼šè¿™è¡Œä»£ç è®¡ç®—äº†çº¿æ€§å‡½æ•°çš„è¾“å‡ºï¼Œé€šè¿‡çŸ©é˜µä¹˜æ³•tf.matmulå°†Wå’ŒXç›¸ä¹˜ï¼Œç„¶ååŠ ä¸Šbå¾—åˆ°ç»“æœYã€‚

æœ€åï¼Œå‡½æ•°è¿”å›è¾“å‡ºå¼ é‡Yä½œä¸ºç»“æœã€‚

è¯·æ³¨æ„ï¼Œè¿™æ®µä»£ç ä½¿ç”¨äº†TensorFlowåº“æ¥åˆ›å»ºå’Œè®¡ç®—å¼ é‡ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œtf.constantç”¨äºåˆ›å»ºå¸¸é‡å¼ é‡ï¼Œtf.matmulç”¨äºçŸ©é˜µä¹˜æ³•æ“ä½œã€‚


# GRADED FUNCTION: linear_function
```python
def linear_function():
    """
    Implements a linear function: 
            Initializes X to be a random tensor of shape (3,1)
            Initializes W to be a random tensor of shape (4,3)
            Initializes b to be a random tensor of shape (4,1)
    Returns: 
    result -- Y = WX + b 
    """

    np.random.seed(1)
    
    """
    Note, to ensure that the "random" numbers generated match the expected results,
    please create the variables in the order given in the starting code below.
    (Do not re-arrange the order).
    """
    # (approx. 4 lines)
    # X = ...
    # W = ...
    # b = ...
    # Y = ...
    # YOUR CODE STARTS HERE
    X = tf.constant(np.random.randn(3,1), name = "X")
    W = tf.constant(np.random.randn(4,3), name = "W")
    b = tf.constant(np.random.randn(4,1), name = "b")
    Y = tf.matmul(W, X) + b
    # YOUR CODE ENDS HERE
    return Y
```
![3](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/89557687-bfa1-4001-9ed0-fe26c5ffe665)

<a name='2-2'></a>
### 2.2 - Computing the Sigmoid 
Amazing! You just implemented a linear function. TensorFlow offers a variety of commonly used neural network functions like `tf.sigmoid` and `tf.softmax`.

For this exercise, compute the sigmoid of z. 

In this exercise, you will: Cast your tensor to type `float32` using `tf.cast`, then compute the sigmoid using `tf.keras.activations.sigmoid`. 

<a name='ex-2'></a>
### Exercise 2 - sigmoid

Implement the sigmoid function below. You should use the following: 

- `tf.cast("...", tf.float32)`
- `tf.keras.activations.sigmoid("...")`

<a name='2-2'></a>
### 2.2 - è®¡ç®—Sigmoidå‡½æ•° 
å¤ªæ£’äº†ï¼ä½ åˆšåˆšå®ç°äº†ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚TensorFlowæä¾›äº†å„ç§å¸¸ç”¨çš„ç¥ç»ç½‘ç»œå‡½æ•°ï¼Œå¦‚`tf.sigmoid`å’Œ`tf.softmax`ã€‚

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œè®¡ç®—zçš„sigmoidã€‚

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å°† ä½¿ç”¨`tf.cast`å°†å¼ é‡è½¬æ¢ä¸º`float32`ç±»å‹ï¼Œç„¶åä½¿ç”¨`tf.keras.activations.sigmoid`è®¡ç®—sigmoidã€‚

<a name='ex-2'></a>
### ç»ƒä¹  2 - sigmoid

å®ç°ä¸‹é¢çš„sigmoidå‡½æ•°ã€‚ä½ åº”è¯¥ä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•ï¼š 

- `tf.cast("...",tf.float32)`ã€‚
- `tf.keras.activations.sigmoid("...")`ã€‚


# GRADED FUNCTION: sigmoid
```python

def sigmoid(z):
    
    """
    Computes the sigmoid of z
    
    Arguments:
    z -- input value, scalar or vector
    
    Returns: 
    a -- (tf.float32) the sigmoid of z
    """
    # tf.keras.activations.sigmoid requires float16, float32, float64, complex64, or complex128.
    
    # (approx. 2 lines)
    # z = ...
    # a = ...
    # YOUR CODE STARTS HERE
    z = tf.cast(z, tf.float32)
    a = tf.keras.activations.sigmoid(z)

    
    # YOUR CODE ENDS HERE
    return a
```


