# åœ¨ä¸ä½¿ç”¨é«˜çº§API kerasçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è‡ªå®šä¹‰å‡½æ•°æ„å»ºç¥ç»ç½‘ç»œ.

Use tf.Variable to modify the state of a variable
Explain the difference between a variable and a constant
Train a Neural Network on a TensorFlow dataset
Programming frameworks like TensorFlow not only cut down on time spent coding, but can also perform optimizations that speed up the code itself.

## Table of Contents
- [1- Packages](#1)
    - [1.1 - Checking TensorFlow Version](#1-1)
- [2 - Basic Optimization with GradientTape](#2)
    - [2.1 - Linear Function](#2-1)
        - [Exercise 1 - linear_function](#ex-1)
    - [2.2 - Computing the Sigmoid](#2-2)
        - [Exercise 2 - sigmoid](#ex-2)
    - [2.3 - Using One Hot Encodings](#2-3)
        - [Exercise 3 - one_hot_matrix](#ex-3)
    - [2.4 - Initialize the Parameters](#2-4)
        - [Exercise 4 - initialize_parameters](#ex-4)
- [3 - Building Your First Neural Network in TensorFlow](#3)
    - [3.1 - Implement Forward Propagation](#3-1)
        - [Exercise 5 - forward_propagation](#ex-5)
    - [3.2 Compute the Total Loss](#3-2)
        - [Exercise 6 - compute_total_loss](#ex-6)
    - [3.3 - Train the Model](#3-3)
- [4 - Bibliography](#4)

<a name='1'></a>
## 1 - Packages

        import h5py
        import numpy as np
        import tensorflow as tf
        import matplotlib.pyplot as plt
        from tensorflow.python.framework.ops import EagerTensor
        from tensorflow.python.ops.resource_variable_ops import ResourceVariable
        import time

<a name='1-1'></a>
### 1.1 - Checking TensorFlow Version 

You will be using v2.3 for this assignment, for maximum speed and efficiency.
![1](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/053ca842-502b-4262-a3af-e74158f2fe14)
<a name='2'></a>
## 2 - Basic Optimization with GradientTape

The beauty of TensorFlow 2 is in its simplicity. Basically, all you need to do is implement forward propagation through a computational graph. TensorFlow will compute the derivatives for you, by moving backwards through the graph recorded with `GradientTape`. All that's left for you to do then is specify the cost function and optimizer you want to use! 

When writing a TensorFlow program, the main object to get used and transformed is the `tf.Tensor`. These tensors are the TensorFlow equivalent of Numpy arrays, i.e. multidimensional arrays of a given data type that also contain information about the computational graph.

Below, you'll use `tf.Variable` to store the state of your variables. Variables can only be created once as its initial value defines the variable shape and type. Additionally, the `dtype` arg in `tf.Variable` can be set to allow data to be converted to that type. But if none is specified, either the datatype will be kept if the initial value is a Tensor, or `convert_to_tensor` will decide. It's generally best for you to specify directly, so nothing breaks!
<a name='2'></a>
## 2 - æ¢¯åº¦å¸¦çš„åŸºæœ¬ä¼˜åŒ–

TensorFlow 2çš„é­…åŠ›åœ¨äºå…¶ç®€å•æ€§ã€‚åŸºæœ¬ä¸Šï¼Œæ‚¨éœ€è¦åšçš„å°±æ˜¯é€šè¿‡è®¡ç®—å›¾å®ç°å‰å‘ä¼ æ’­ã€‚TensorFlowå°†é€šè¿‡ "GradientTape "è®°å½•çš„å›¾å‘åç§»åŠ¨ï¼Œä¸ºæ‚¨è®¡ç®—å¯¼æ•°ã€‚æ‚¨æ‰€è¦åšçš„å°±æ˜¯æŒ‡å®šæ‚¨è¦ä½¿ç”¨çš„ä»£ä»·å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼

åœ¨ç¼–å†™TensorFlowç¨‹åºæ—¶ï¼Œä½¿ç”¨å’Œè½¬æ¢çš„ä¸»è¦å¯¹è±¡æ˜¯`tf.Tensor`ã€‚è¿™äº›å¼ é‡ç›¸å½“äºTensorFlowçš„Numpyæ•°ç»„ï¼Œå³ç»™å®šæ•°æ®ç±»å‹çš„å¤šç»´æ•°ç»„ï¼ŒåŒæ—¶åŒ…å«è®¡ç®—å›¾çš„ä¿¡æ¯ã€‚

ä¸‹é¢ï¼Œæ‚¨å°†ä½¿ç”¨`tf.Variable`æ¥å­˜å‚¨å˜é‡çš„çŠ¶æ€ã€‚å˜é‡åªèƒ½åˆ›å»ºä¸€æ¬¡ï¼Œå› ä¸ºå®ƒçš„åˆå§‹å€¼å®šä¹‰äº†å˜é‡çš„å½¢çŠ¶å’Œç±»å‹ã€‚æ­¤å¤–ï¼Œå¯ä»¥è®¾ç½®`tf.Variable`ä¸­çš„`dtype`å‚æ•°ï¼Œä»¥ä¾¿å°†æ•°æ®è½¬æ¢ä¸ºè¯¥ç±»å‹ã€‚ä½†å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œå¦‚æœåˆå§‹å€¼æ˜¯å¼ é‡ï¼Œæ•°æ®ç±»å‹å°†è¢«ä¿ç•™ï¼Œæˆ–è€…ç”±`convert_to_tensor`å†³å®šã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæœ€å¥½æ˜¯ç›´æ¥æŒ‡å®šï¼Œè¿™æ ·å°±ä¸ä¼šå‡ºé”™ï¼


Here you'll call the TensorFlow dataset created on a HDF5 file, which you can use in place of a Numpy array to store your datasets. You can think of this as a TensorFlow data generator! 

You will use the Hand sign data set, that is composed of images with shape 64x64x3.

è¿™é‡Œä½ å°†è°ƒç”¨åœ¨HDF5æ–‡ä»¶ä¸Šåˆ›å»ºçš„TensorFlowæ•°æ®é›†ï¼Œä½ å¯ä»¥ç”¨å®ƒæ¥ä»£æ›¿Numpyæ•°ç»„æ¥å­˜å‚¨ä½ çš„æ•°æ®é›†ã€‚æ‚¨å¯ä»¥å°†å…¶è§†ä¸ºTensorFlowæ•°æ®ç”Ÿæˆå™¨ï¼

æ‚¨å°†ä½¿ç”¨æ‰‹åŠ¿æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±å½¢çŠ¶ä¸º64x64x3çš„å›¾åƒç»„æˆã€‚

## åŠ è½½æ•°æ®ï¼Œå¹¶æŠŠæ•°æ®è½¬æ¢æˆTensorflowéœ€è¦çš„æ ·å­

è¿™æ®µä»£ç ä½¿ç”¨äº†h5pyåº“æ¥è¯»å–HDF5æ–‡ä»¶æ ¼å¼ä¸­çš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚

h5py.File('datasets/train_signs.h5', "r")ï¼šè¿™è¡Œä»£ç æ‰“å¼€åä¸ºtrain_signs.h5çš„HDF5æ–‡ä»¶ï¼Œå¹¶ä»¥åªè¯»æ¨¡å¼æ‰“å¼€å®ƒã€‚HDF5æ˜¯ä¸€ç§ç”¨äºå­˜å‚¨å’Œç»„ç»‡å¤§å‹æ•°æ®é›†çš„æ–‡ä»¶æ ¼å¼ï¼Œé€šå¸¸åœ¨æœºå™¨å­¦ä¹ ä¸­ç”¨äºå­˜å‚¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚

h5py.File('datasets/test_signs.h5', "r")ï¼šè¿™è¡Œä»£ç æ‰“å¼€åä¸ºtest_signs.h5çš„HDF5æ–‡ä»¶ï¼Œå¹¶ä»¥åªè¯»æ¨¡å¼æ‰“å¼€å®ƒï¼Œç”¨äºå­˜å‚¨æµ‹è¯•æ•°æ®ã€‚

ä¸€æ—¦è¿™ä¸¤ä¸ªHDF5æ–‡ä»¶è¢«æ‰“å¼€ï¼Œä½ å°±å¯ä»¥ä½¿ç”¨train_datasetå’Œtest_datasetä¸¤ä¸ªå˜é‡æ¥è®¿é—®å…¶ä¸­çš„æ•°æ®é›†å’Œç›¸å…³ä¿¡æ¯ã€‚é€šå¸¸ï¼Œè¿™äº›æ–‡ä»¶ä¼šåŒ…å«è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„ç‰¹å¾ï¼ˆä¾‹å¦‚å›¾åƒæ•°æ®ï¼‰ä»¥åŠå¯¹åº”çš„æ ‡ç­¾ï¼ˆä¾‹å¦‚å›¾åƒæ‰€å±çš„ç±»åˆ«ï¼‰ã€‚

è¦è¿›ä¸€æ­¥ä½¿ç”¨è¿™äº›æ•°æ®é›†ï¼Œä½ å¯ä»¥é€šè¿‡h5pyåº“ä¸­çš„æ–¹æ³•æ¥è·å–å…¶ä¸­çš„æ•°æ®å’Œå…ƒæ•°æ®ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ç±»ä¼¼train_dataset['features']çš„æ–¹å¼æ¥è·å–è®­ç»ƒæ•°æ®é›†ä¸­çš„ç‰¹å¾æ•°æ®ï¼Œtrain_dataset['labels']æ¥è·å–è®­ç»ƒæ•°æ®é›†ä¸­çš„æ ‡ç­¾æ•°æ®ï¼Œç­‰ç­‰ã€‚
```
train_dataset = h5py.File('datasets/train_signs.h5', "r")
test_dataset = h5py.File('datasets/test_signs.h5', "r")
```
**è¿›è¡Œæå–å’Œè½¬æ¢**

è¿™æ®µä»£ç ä½¿ç”¨TensorFlowä¸­çš„tf.data.Dataset.from_tensor_slicesæ–¹æ³•ä»numpyæ•°ç»„ï¼ˆæˆ–å¼ é‡ï¼‰ä¸­åˆ›å»ºäº†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†çš„tf.data.Datasetå¯¹è±¡ã€‚

train_dataset['train_set_x']ï¼šè¿™éƒ¨åˆ†ä»£ç ä»train_datasetå¯¹è±¡ä¸­è·å–äº†åä¸ºtrain_set_xçš„æ•°æ®ï¼Œè¿™å¯èƒ½æ˜¯è®­ç»ƒæ•°æ®é›†çš„ç‰¹å¾æ•°æ®ï¼ˆä¾‹å¦‚å›¾åƒæ•°æ®ï¼‰ã€‚

train_dataset['train_set_y']ï¼šè¿™éƒ¨åˆ†ä»£ç ä»train_datasetå¯¹è±¡ä¸­è·å–äº†åä¸ºtrain_set_yçš„æ•°æ®ï¼Œè¿™å¯èƒ½æ˜¯è®­ç»ƒæ•°æ®é›†çš„æ ‡ç­¾æ•°æ®ï¼ˆä¾‹å¦‚å›¾åƒç±»åˆ«æ ‡ç­¾ï¼‰ã€‚

test_dataset['test_set_x']ï¼šè¿™éƒ¨åˆ†ä»£ç ä»test_datasetå¯¹è±¡ä¸­è·å–äº†åä¸ºtest_set_xçš„æ•°æ®ï¼Œè¿™å¯èƒ½æ˜¯æµ‹è¯•æ•°æ®é›†çš„ç‰¹å¾æ•°æ®ã€‚

test_dataset['test_set_y']ï¼šè¿™éƒ¨åˆ†ä»£ç ä»test_datasetå¯¹è±¡ä¸­è·å–äº†åä¸ºtest_set_yçš„æ•°æ®ï¼Œè¿™å¯èƒ½æ˜¯æµ‹è¯•æ•°æ®é›†çš„æ ‡ç­¾æ•°æ®ã€‚
```python
x_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_x'])
y_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_y'])

x_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_x'])
y_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_y'])
```

Since TensorFlow Datasets are generators, you can't access directly the contents unless you iterate over them in a for loop, or by explicitly creating a Python iterator using `iter` and consuming its
elements using `next`. Also, you can inspect the `shape` and `dtype` of each element using the `element_spec` attribute.

ç”±äºTensorFlowæ•°æ®é›†æ˜¯ç”Ÿæˆå™¨ï¼Œä½ ä¸èƒ½ç›´æ¥è®¿é—®å…¶å†…å®¹ï¼Œé™¤éä½ åœ¨ä¸€ä¸ªforå¾ªç¯ä¸­éå†å®ƒä»¬ï¼Œæˆ–è€…é€šè¿‡ä½¿ç”¨`iter`æ˜¾å¼åœ°åˆ›å»ºä¸€ä¸ªPythonè¿­ä»£å™¨ï¼Œå¹¶ä½¿ç”¨`next`æ¶ˆè€—å®ƒçš„
å…ƒç´ ã€‚å¦å¤–ï¼Œä½ å¯ä»¥ä½¿ç”¨`element_spec`å±æ€§æ£€æŸ¥æ¯ä¸ªå…ƒç´ çš„`shape`å’Œ`dtype`ã€‚
![2](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/479da84a-4625-4a23-b241-30955be6ef2c)


The dataset that you'll be using during this assignment is a subset of the sign language digits. It contains six different classes representing the digits from 0 to 5.

æœ¬ä½œä¸šä¸­æ‚¨å°†ä½¿ç”¨çš„æ•°æ®é›†æ˜¯æ‰‹è¯­æ•°å­—çš„å­é›†ã€‚å®ƒåŒ…å«å…­ä¸ªä¸åŒçš„ç±»åˆ«ï¼Œä»£è¡¨ä»0åˆ°5çš„æ•°å­—ã€‚


è¿™æ®µä»£ç é€šè¿‡éå†TensorFlow Datasetä¸­çš„y_trainï¼ˆå‡è®¾y_trainæ˜¯ä¸€ä¸ªTensorFlow Datasetå¯¹è±¡ï¼‰æ¥è·å–è®­ç»ƒæ•°æ®é›†ä¸­çš„å”¯ä¸€æ ‡ç­¾ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ä¸€ä¸ªé›†åˆï¼ˆunique_labelsï¼‰ä¸­ã€‚

unique_labels = set()ï¼šè¿™ä¸€è¡Œåˆ›å»ºäº†ä¸€ä¸ªç©ºçš„é›†åˆï¼Œç”¨äºå­˜å‚¨å”¯ä¸€çš„æ ‡ç­¾ã€‚

for element in y_train:ï¼šè¿™ä¸ªå¾ªç¯éå†äº†y_trainä¸­çš„æ¯ä¸ªå…ƒç´ ã€‚

unique_labels.add(element.numpy())ï¼šåœ¨å¾ªç¯ä¸­ï¼Œä»£ç ä½¿ç”¨addæ–¹æ³•å°†æ¯ä¸ªå…ƒç´ ï¼ˆå‡è®¾æ˜¯ä¸€ä¸ªTensorï¼‰çš„numpyè¡¨ç¤ºï¼ˆå³å…ƒç´ çš„å®é™…å€¼ï¼‰æ·»åŠ åˆ°unique_labelsé›†åˆä¸­ã€‚è¿™æ ·ï¼Œé›†åˆunique_labelså°±ä¼šåŒ…å«è®­ç»ƒæ•°æ®é›†ä¸­çš„æ‰€æœ‰å”¯ä¸€æ ‡ç­¾å€¼ã€‚

print(unique_labels)ï¼šæœ€åï¼Œä»£ç æ‰“å°è¾“å‡ºäº†unique_labelsé›†åˆï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒæ•°æ®é›†ä¸­çš„æ‰€æœ‰å”¯ä¸€æ ‡ç­¾å€¼ã€‚

æ€»ä¹‹ï¼Œè¿™æ®µä»£ç çš„ä½œç”¨æ˜¯è·å–è®­ç»ƒæ•°æ®é›†y_trainä¸­çš„æ‰€æœ‰å”¯ä¸€æ ‡ç­¾ï¼Œå¹¶å°†å…¶æ‰“å°è¾“å‡ºã€‚
```python
unique_labels = set()
for element in y_train:
    unique_labels.add(element.numpy())
print(unique_labels)
```

![1](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/c9e673dd-75a3-48ba-8d8d-d4a8403a7a71)

You can see some of the images in the dataset by running the following cell.

æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å•å…ƒæ ¼æŸ¥çœ‹æ•°æ®é›†ä¸­çš„éƒ¨åˆ†å›¾åƒã€‚

è¿™æ®µä»£ç ä½¿ç”¨matplotlibåº“åœ¨ä¸€ä¸ª5x5çš„å›¾åƒç½‘æ ¼ä¸­æ˜¾ç¤ºè®­ç»ƒæ•°æ®é›†ä¸­çš„å›¾åƒå’Œå¯¹åº”çš„æ ‡ç­¾ã€‚

images_iter = iter(x_train)ï¼šè¿™è¡Œä»£ç å°†è®­ç»ƒæ•°æ®é›†x_trainè½¬æ¢ä¸ºä¸€ä¸ªè¿­ä»£å™¨å¯¹è±¡images_iterã€‚è¿­ä»£å™¨å¯ä»¥ç”¨äºé€ä¸ªè®¿é—®æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚

labels_iter = iter(y_train)ï¼šè¿™è¡Œä»£ç å°†è®­ç»ƒæ•°æ®é›†y_trainè½¬æ¢ä¸ºä¸€ä¸ªè¿­ä»£å™¨å¯¹è±¡labels_iterã€‚åŒæ ·ï¼Œè¿­ä»£å™¨å¯ä»¥ç”¨äºé€ä¸ªè®¿é—®æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚

plt.figure(figsize=(10, 10))ï¼šè¿™è¡Œä»£ç åˆ›å»ºä¸€ä¸ªå¤§å°ä¸º10x10çš„æ–°å›¾åƒçª—å£ã€‚

for i in range(25):ï¼šè¿™ä¸ªå¾ªç¯éå†25æ¬¡ï¼Œå³åœ¨å›¾åƒç½‘æ ¼ä¸­æ˜¾ç¤º25å¼ å›¾åƒã€‚

ax = plt.subplot(5, 5, i + 1)ï¼šè¿™è¡Œä»£ç åˆ›å»ºä¸€ä¸ªå­å›¾ï¼Œå°†å½“å‰å›¾åƒæ”¾åœ¨5x5çš„ç½‘æ ¼ä¸­çš„ç¬¬i+1ä¸ªä½ç½®ã€‚

plt.imshow(next(images_iter).numpy().astype("uint8"))ï¼šè¿™è¡Œä»£ç ä½¿ç”¨next(images_iter)è·å–è®­ç»ƒæ•°æ®é›†ä¸­çš„ä¸‹ä¸€ä¸ªå›¾åƒï¼Œå¹¶ä½¿ç”¨imshowæ–¹æ³•æ˜¾ç¤ºå›¾åƒã€‚.numpy()æ–¹æ³•å°†TensorFlowå¼ é‡è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œ.astype("uint8")å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ— ç¬¦å·8ä½æ•´æ•°ã€‚

plt.title(next(labels_iter).numpy().astype("uint8"))ï¼šè¿™è¡Œä»£ç ä½¿ç”¨next(labels_iter)è·å–è®­ç»ƒæ•°æ®é›†ä¸­ä¸‹ä¸€ä¸ªå›¾åƒå¯¹åº”çš„æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨titleæ–¹æ³•å°†å…¶æ˜¾ç¤ºä¸ºå›¾åƒçš„æ ‡é¢˜ã€‚åŒæ ·ï¼Œ.numpy()æ–¹æ³•å°†TensorFlowå¼ é‡è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œ.astype("uint8")å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ— ç¬¦å·8ä½æ•´æ•°ã€‚

plt.axis("off")ï¼šè¿™è¡Œä»£ç å…³é—­å›¾åƒçš„åæ ‡è½´æ˜¾ç¤ºï¼Œä»¥ä¾¿æ›´å¥½åœ°æŸ¥çœ‹å›¾åƒæœ¬èº«ã€‚

```python
images_iter = iter(x_train)
labels_iter = iter(y_train)
plt.figure(figsize=(10, 10))
for i in range(25):
    ax = plt.subplot(5, 5, i + 1)
    plt.imshow(next(images_iter).numpy().astype("uint8"))
    plt.title(next(labels_iter).numpy().astype("uint8"))
    plt.axis("off")
```
![5](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/e6cb31aa-333e-49da-9915-161ef115448e)

TensorFlowæ•°æ®é›†å’ŒNumpyæ•°ç»„ä¹‹é—´è¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„åŒºåˆ«ï¼š å¦‚æœæ‚¨éœ€è¦è½¬æ¢æ•°æ®é›†ï¼Œæ‚¨éœ€è¦è°ƒç”¨`map`æ–¹æ³•ï¼Œå°†ä½œä¸ºå‚æ•°ä¼ é€’çš„å‡½æ•°åº”ç”¨åˆ°æ¯ä¸ªå…ƒç´ ä¸Šã€‚

## Normalization the images 

å’Œåœ¨Numpyä¸­çš„å¸¸è§„æ“ä½œç›¸åŒï¼Œåƒç´ çš„æœ€å¤§å€¼çš„255ï¼Œæˆ‘ä»¬è®©æ¯ä¸€ä¸ªdim_layerä¸­çš„å…ƒç´ éƒ½é™¤ä»¥255ï¼Œå½’ä¸€åŒ–åˆ°[0,1]çš„èŒƒå›´ä¸­ï¼Œç„¶åå°†3ç»´arrayï¼Œ
è½¬æ¢æˆ1ç»´arrayï¼ˆ64*64*3ï¼‰.

è€Œåœ¨TensorFlow çš„ä¸Šé¢ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æŠŠå°†å›¾åƒåƒç´ å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»å‹ã€‚å†å°†åƒç´ å€¼é™¤ä»¥255ï¼Œå°†å…¶å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´å†…ã€‚

è€Œä»£ç å’ŒNumpyä¸­æ—¶ï¼Œæœ‰æ‰€ä¸åŒ.
è¿™æ˜¯Numpyä¸­ï¼š image = image.astype(np.float32) / 255.0
è¿™æ˜¯Tfä¸­ï¼š    image = tf.cast(image, tf.float32) / 255.0

ç„¶åå°†è¿™ä¸ª3ç»´æ•°ç»„è½¬æˆ1ç»´ï¼š64*64*3
ä½¿ç”¨reshapeï¼ˆï¼‰å‡½æ•°è¾¾åˆ°æ•ˆæœ    image = tf.reshape(image, [-1,])
```python
def normalize(image):
    """
    Transform an image into a tensor of shape (64 * 64 * 3, )
    and normalize its components.
    
    Arguments
    image - Tensor.
    
    Returns: 
    result -- Transformed tensor 
    """
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.reshape(image, [-1,])
    return image
```
ç„¶åè¿™æ®µä»£ç ä½¿ç”¨äº†TensorFlowä¸­çš„mapæ–¹æ³•å¯¹è®­ç»ƒæ•°æ®é›†x_trainå’Œæµ‹è¯•æ•°æ®é›†x_testä¸­çš„æ¯ä¸ªå›¾åƒåº”ç”¨normalizeå‡½æ•°è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚

x_trainå’Œx_testæ˜¯TensorFlowçš„tf.data.Datasetå¯¹è±¡ï¼Œè¡¨ç¤ºè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ã€‚

normalizeæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ˜¯å°†å›¾åƒè¿›è¡Œå½’ä¸€åŒ–çš„å‡½æ•°ï¼Œå°†åƒç´ å€¼é™¤ä»¥255æ¥å°†åƒç´ å€¼ç¼©æ”¾åˆ°[0, 1]çš„èŒƒå›´å†…ã€‚

new_train = x_train.map(normalize)ï¼šè¿™è¡Œä»£ç ä½¿ç”¨mapæ–¹æ³•ï¼Œå°†normalizeå‡½æ•°åº”ç”¨äºx_trainæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒã€‚è¿™æ ·ï¼Œè®­ç»ƒæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒéƒ½ä¼šè¢«å½’ä¸€åŒ–å¤„ç†ï¼Œå¹¶å­˜å‚¨åœ¨new_trainæ•°æ®é›†ä¸­ã€‚

new_test = x_test.map(normalize)ï¼šè¿™è¡Œä»£ç ä½¿ç”¨mapæ–¹æ³•ï¼Œå°†normalizeå‡½æ•°åº”ç”¨äºx_testæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒã€‚è¿™æ ·ï¼Œæµ‹è¯•æ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒä¹Ÿä¼šè¢«å½’ä¸€åŒ–å¤„ç†ï¼Œå¹¶å­˜å‚¨åœ¨new_testæ•°æ®é›†ä¸­ã€‚

æœ€ç»ˆï¼Œnew_trainå’Œnew_testæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒéƒ½è¢«å½’ä¸€åŒ–å¤„ç†ï¼Œä»¥ä¾¿åç»­çš„å›¾åƒå¤„ç†å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€‚è¿™æ˜¯åˆ©ç”¨tf.data.Dataset.map()æ–¹æ³•å¯¹æ•°æ®é›†ä¸­çš„å…ƒç´ è¿›è¡Œé¢„å¤„ç†çš„å¸¸è§ç”¨æ³•ã€‚
```
new_train = x_train.map(normalize)
new_test = x_test.map(normalize)
```
ä½¿ç”¨element_specåç¼€æ¥æŸ¥è¯¢æ–°å¤åˆ¶çš„è®­ç»ƒæ•°ç»„çš„å†…å®¹
```
new_train.element_spec
```
![1](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/759f930a-fb1c-413b-b177-7be48855ebde)
![2](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/56a1e4fe-812c-40ff-9ca2-2cf99e14cbc7)

<a name='2-1'></a>
### 2.1 - Linear Function

Let's begin this programming exercise by computing the following equation: $Y = WX + b$, where $W$ and $X$ are random matrices and b is a random vector. 

è®©æˆ‘ä»¬ä»è®¡ç®—ä¸‹åˆ—æ–¹ç¨‹å¼€å§‹è¿™ä¸ªç¼–ç¨‹ç»ƒä¹ ï¼šğ‘Œ=ğ‘‹+ğ‘ ï¼Œå…¶ä¸­ ğ‘‹ å’Œ ğ‘‹ æ˜¯éšæœºçŸ©é˜µï¼Œb æ˜¯éšæœºå‘é‡ã€‚

<a name='ex-1'></a>
### Exercise 1 - linear_function

Compute $WX + b$ where $W, X$, and $b$ are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1). As an example, this is how to define a constant X with the shape (3,1):
```python
X = tf.constant(np.random.randn(3,1), name = "X")

```
Note that the difference between `tf.constant` and `tf.Variable` is that you can modify the state of a `tf.Variable` but cannot change the state of a `tf.constant`.

You might find the following functions helpful: 
- tf.matmul(..., ...) to do a matrix multiplication
- tf.add(..., ...) to do an addition
- np.random.randn(...) to initialize randomly

 ç»ƒä¹  1 - çº¿æ€§å‡½æ•°
è®¡ç®— ğ‘‹+ğ‘ï¼Œå…¶ä¸­ğ‘‹, ğ‘å’Œ ğ‘ä»éšæœºæ­£æ€åˆ†å¸ƒä¸­æŠ½å–ã€‚W çš„å½¢çŠ¶ä¸º (4,3)ï¼ŒX ä¸º (3,1)ï¼Œb ä¸º (4,1)ã€‚ä¸¾ä¾‹è¯´æ˜ï¼Œå¦‚ä½•å®šä¹‰å½¢çŠ¶ä¸º(3,1)çš„å¸¸æ•°Xï¼š

X = tf.constant(np.random.randn(3,1), name = "X")
è¯·æ³¨æ„ï¼Œtf.constantå’Œtf.Variableçš„åŒºåˆ«åœ¨äºï¼Œæ‚¨å¯ä»¥ä¿®æ”¹tf.Variableçš„çŠ¶æ€ï¼Œä½†ä¸èƒ½æ”¹å˜tf.constantçš„çŠ¶æ€ã€‚

æ‚¨å¯èƒ½ä¼šå‘ç°ä»¥ä¸‹å‡½æ•°å¾ˆæœ‰ç”¨ï¼š

tf.matmul(...,...)è¿›è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—
tf.add(...,...)è¿›è¡ŒåŠ æ³•è¿ç®—
np.random.randn(...)ç”¨äºéšæœºåˆå§‹åŒ–


è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œæ ¹æ®ç»™å®šçš„åˆå§‹åŒ–è§„åˆ™åˆ›å»ºäº†éšæœºå¼ é‡ï¼Œå¹¶è®¡ç®—å‡ºçº¿æ€§å‡½æ•°çš„è¾“å‡ºã€‚

np.random.seed(1)ï¼šè¿™è¡Œä»£ç è®¾ç½®äº†éšæœºç§å­ï¼Œä»¥ç¡®ä¿éšæœºæ•°çš„ç”Ÿæˆä¸é¢„æœŸç»“æœä¸€è‡´ã€‚

X = tf.constant(np.random.randn(3,1), name = "X")ï¼šè¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªåä¸ºXçš„å¸¸é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º(3, 1)ï¼Œå€¼ä¸ºéšæœºç”Ÿæˆçš„æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ•°å­—ã€‚

W = tf.constant(np.random.randn(4,3), name = "W")ï¼šè¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªåä¸ºWçš„å¸¸é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º(4, 3)ï¼Œå€¼ä¸ºéšæœºç”Ÿæˆçš„æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ•°å­—ã€‚

b = tf.constant(np.random.randn(4,1), name = "b")ï¼šè¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªåä¸ºbçš„å¸¸é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º(4, 1)ï¼Œå€¼ä¸ºéšæœºç”Ÿæˆçš„æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ•°å­—ã€‚

Y = tf.matmul(W, X) + bï¼šè¿™è¡Œä»£ç è®¡ç®—äº†çº¿æ€§å‡½æ•°çš„è¾“å‡ºï¼Œé€šè¿‡çŸ©é˜µä¹˜æ³•tf.matmulå°†Wå’ŒXç›¸ä¹˜ï¼Œç„¶ååŠ ä¸Šbå¾—åˆ°ç»“æœYã€‚

æœ€åï¼Œå‡½æ•°è¿”å›è¾“å‡ºå¼ é‡Yä½œä¸ºç»“æœã€‚

è¯·æ³¨æ„ï¼Œè¿™æ®µä»£ç ä½¿ç”¨äº†TensorFlowåº“æ¥åˆ›å»ºå’Œè®¡ç®—å¼ é‡ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œtf.constantç”¨äºåˆ›å»ºå¸¸é‡å¼ é‡ï¼Œtf.matmulç”¨äºçŸ©é˜µä¹˜æ³•æ“ä½œã€‚


# GRADED FUNCTION: linear_function
```python
def linear_function():
    """
    Implements a linear function: 
            Initializes X to be a random tensor of shape (3,1)
            Initializes W to be a random tensor of shape (4,3)
            Initializes b to be a random tensor of shape (4,1)
    Returns: 
    result -- Y = WX + b 
    """

    np.random.seed(1)
    
    """
    Note, to ensure that the "random" numbers generated match the expected results,
    please create the variables in the order given in the starting code below.
    (Do not re-arrange the order).
    """
    # (approx. 4 lines)
    # X = ...
    # W = ...
    # b = ...
    # Y = ...
    # YOUR CODE STARTS HERE
    X = tf.constant(np.random.randn(3,1), name = "X")
    W = tf.constant(np.random.randn(4,3), name = "W")
    b = tf.constant(np.random.randn(4,1), name = "b")
    Y = tf.matmul(W, X) + b
    # YOUR CODE ENDS HERE
    return Y
```
![3](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/89557687-bfa1-4001-9ed0-fe26c5ffe665)

<a name='2-2'></a>
### 2.2 - Computing the Sigmoid 
Amazing! You just implemented a linear function. TensorFlow offers a variety of commonly used neural network functions like `tf.sigmoid` and `tf.softmax`.

For this exercise, compute the sigmoid of z. 

In this exercise, you will: Cast your tensor to type `float32` using `tf.cast`, then compute the sigmoid using `tf.keras.activations.sigmoid`. 

<a name='ex-2'></a>
### Exercise 2 - sigmoid

Implement the sigmoid function below. You should use the following: 

- `tf.cast("...", tf.float32)`
- `tf.keras.activations.sigmoid("...")`

<a name='2-2'></a>
### 2.2 - è®¡ç®—Sigmoidå‡½æ•° 
å¤ªæ£’äº†ï¼ä½ åˆšåˆšå®ç°äº†ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚TensorFlowæä¾›äº†å„ç§å¸¸ç”¨çš„ç¥ç»ç½‘ç»œå‡½æ•°ï¼Œå¦‚`tf.sigmoid`å’Œ`tf.softmax`ã€‚

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œè®¡ç®—zçš„sigmoidã€‚

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å°† ä½¿ç”¨`tf.cast`å°†å¼ é‡è½¬æ¢ä¸º`float32`ç±»å‹ï¼Œç„¶åä½¿ç”¨`tf.keras.activations.sigmoid`è®¡ç®—sigmoidã€‚

<a name='ex-2'></a>
### ç»ƒä¹  2 - sigmoid

å®ç°ä¸‹é¢çš„sigmoidå‡½æ•°ã€‚ä½ åº”è¯¥ä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•ï¼š 

- `tf.cast("...",tf.float32)`ã€‚
- `tf.keras.activations.sigmoid("...")`ã€‚


# GRADED FUNCTION: sigmoid
```python

def sigmoid(z):
    
    """
    Computes the sigmoid of z
    
    Arguments:
    z -- input value, scalar or vector
    
    Returns: 
    a -- (tf.float32) the sigmoid of z
    """
    # tf.keras.activations.sigmoid requires float16, float32, float64, complex64, or complex128.
    
    # (approx. 2 lines)
    # z = ...
    # a = ...
    # YOUR CODE STARTS HERE
    z = tf.cast(z, tf.float32)
    a = tf.keras.activations.sigmoid(z)

    
    # YOUR CODE ENDS HERE
    return a
```

![1](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/9e902533-c801-492c-8110-5a3ebb481cde)

<a name='2-3'></a>
### 2.3 - Using One Hot Encodings

Many times in deep learning you will have a $Y$ vector with numbers ranging from $0$ to $C-1$, where $C$ is the number of classes. If $C$ is for example 4, then you might have the following y vector which you will need to convert like this:
![2](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/10a1ed1e-85c3-47b7-871d-082dfd6291fb)

This is called "one hot" encoding, because in the converted representation, exactly one element of each column is "hot" (meaning set to 1). To do this conversion in numpy, you might have to write a few lines of code. In TensorFlow, you can use one line of code: 

- [tf.one_hot(labels, depth, axis=0)](https://www.tensorflow.org/api_docs/python/tf/one_hot)

`axis=0` indicates the new axis is created at dimension 0

<a name='ex-3'></a>
### Exercise 3 - one_hot_matrix

Implement the function below to take one label and the total number of classes $C$, and return the one hot encoding in a column wise matrix. Use `tf.one_hot()` to do this, and `tf.reshape()` to reshape your one hot tensor! 

- `tf.reshape(tensor, shape)`

2.3 - ä½¿ç”¨å•çƒ­ç¼–ç 
åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ‚¨ç»å¸¸ä¼šæœ‰ä¸€ä¸ªå‘é‡ï¼Œå…¶ä¸­åŒ…å«çš„æ•°å­—ä»åˆ° ï¼Œå…¶ä¸­æ˜¯ç±»çš„æ•°é‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ˜¯4ï¼Œé‚£ä¹ˆæ‚¨å¯èƒ½ä¼šæœ‰å¦‚ä¸‹çš„Yå‘é‡ï¼Œæ‚¨éœ€è¦åƒè¿™æ ·è¿›è¡Œè½¬æ¢ï¼š 


è¿™å°±æ˜¯æ‰€è°“çš„ "one hot "ç¼–ç ï¼Œå› ä¸ºåœ¨è½¬æ¢åçš„è¡¨ç¤ºä¸­ï¼Œæ¯ä¸€åˆ—ä¸­æ­£å¥½æœ‰ä¸€ä¸ªå…ƒç´ æ˜¯ "hot "çš„ï¼ˆæ„æ€æ˜¯è®¾ä¸º1ï¼‰ã€‚è¦åœ¨numpyä¸­è¿›è¡Œè¿™ç§è½¬æ¢ï¼Œæ‚¨å¯èƒ½éœ€è¦ç¼–å†™å‡ è¡Œä»£ç ã€‚åœ¨TensorFlowä¸­ï¼Œæ‚¨åªéœ€è¦å†™ä¸€è¡Œä»£ç å³å¯ï¼š

tf.one_hot(labels, depth, axis=0)
axis=0è¡¨ç¤ºåœ¨ç»´åº¦0åˆ›å»ºæ–°è½´


ç»ƒä¹ 3 - one_hot_matrix
æ‰§è¡Œä¸‹é¢çš„å‡½æ•°ï¼Œè·å–ä¸€ä¸ªæ ‡ç­¾å’Œç±»çš„æ€»æ•°ï¼Œå¹¶ä»¥åˆ—ä¸ºå•ä½çš„çŸ©é˜µè¿”å›one_hotç¼–ç ã€‚ä½¿ç”¨tf.one_hot()æ¥å®ç°è¿™ä¸ªåŠŸèƒ½ï¼Œä½¿ç”¨tf.reshape()æ¥é‡å¡‘ä½ çš„one hotå¼ é‡ï¼



åœ¨ç¥ç»ç½‘ç»œä¸­ï¼ŒOne-Hot ç¼–ç æ˜¯ä¸€ç§å¸¸ç”¨çš„å‘é‡è¡¨ç¤ºæ–¹æ³•ï¼Œç”¨äºè¡¨ç¤ºç¦»æ•£å‹çš„åˆ†ç±»æˆ–æ ‡ç­¾ä¿¡æ¯ã€‚å®ƒå°†æ¯ä¸ªç±»åˆ«æˆ–æ ‡ç­¾æ˜ å°„ä¸ºä¸€ä¸ªç”± 0 å’Œ 1 ç»„æˆçš„å‘é‡ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªå…ƒç´ ä¸º 1ï¼Œå…¶ä»–å…ƒç´ ä¸º 0ã€‚è¿™ä¸ªå…ƒç´ çš„ä½ç½®è¡¨ç¤ºå¯¹åº”çš„ç±»åˆ«æˆ–æ ‡ç­¾ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾æœ‰ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œå…±æœ‰ä¸‰ä¸ªç±»åˆ«ï¼šçŒ«ã€ç‹—å’Œé¸Ÿã€‚ä½¿ç”¨ One-Hot ç¼–ç æ—¶ï¼Œå¯ä»¥å°†å®ƒä»¬è¡¨ç¤ºä¸ºä»¥ä¸‹å‘é‡ï¼š

çŒ«ï¼š[1, 0, 0]
ç‹—ï¼š[0, 1, 0]
é¸Ÿï¼š[0, 0, 1]


**ä»£ç è§£é‡Š**

å…ˆå®šä¹‰äº†ä¸€ä¸ªåä¸º one_hot_matrix çš„å‡½æ•°ï¼Œå®ƒæ¥å—ä¸¤ä¸ªå‚æ•°ï¼šlabelï¼ˆåˆ†ç±»æ ‡ç­¾ï¼‰å’Œ depthï¼ˆç±»åˆ«çš„æ•°é‡ï¼‰ã€‚
```
    one_hot = tf.reshape(tf.one_hot(label, depth, axis=0), shape=[-1, ])
```
è¿™ä¸€è¡Œä»£ç çš„ä½œç”¨æ˜¯è®¡ç®—è¾“å…¥æ ‡ç­¾ label çš„ç‹¬çƒ­ç¼–ç ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ tf.one_hot() å‡½æ•°å°† label ç¼–ç ä¸ºç‹¬çƒ­å‘é‡ï¼Œå…¶ä¸­ depth æŒ‡å®šäº†ç±»åˆ«çš„æ•°é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ tf.reshape() å¯¹ç»“æœè¿›è¡Œå½¢çŠ¶è°ƒæ•´ï¼Œå°†å…¶è½¬æ¢ä¸ºå•åˆ—çŸ©é˜µã€‚shape=[-1, ] è¡¨ç¤ºæˆ‘ä»¬å°†ç»“æœè°ƒæ•´ä¸ºä¸€ä¸ªæœªçŸ¥è¡Œæ•°ã€å•åˆ—çš„å½¢çŠ¶ã€‚


# GRADED FUNCTION: one_hot_matrix
```python
def one_hot_matrix(label, depth=6):
    """
Â Â Â Â Computes the one hot encoding for a single label
Â Â Â Â 
Â Â Â Â Arguments:
        label --  (int) Categorical labels
        depth --  (int) Number of different classes that label can take
Â Â Â Â 
Â Â Â Â Returns:
         one_hot -- tf.Tensor A single-column matrix with the one hot encoding.
    """
    # (approx. 1 line)
    # one_hot = None(None(None, None, None), shape=[-1, ])
    # YOUR CODE STARTS HERE
    one_hot = tf.reshape(tf.one_hot(label, depth, axis = 0),shape =  [-1, ])
    
    # YOUR CODE ENDS HERE
    return one_hot
```
![1](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/029bf760-f8a0-47d1-8bb2-8015b134a622)

**ä¸‹ä¸€æ­¥å°±æ˜¯ä½¿ç”¨.mapå‡½æ•°ä½¿å¾—arrayä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½è¢«one_hot_matrixå‡½æ•°å¤„ç†**
```python
new_y_test = y_test.map(one_hot_matrix)
new_y_train = y_train.map(one_hot_matrix)
```
![2](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/586c9cbc-ed2d-47ff-b7c9-c43e91e917d4)

<a name='2-4'></a>
### 2.4 - Initialize the Parameters 

Now you'll initialize a vector of numbers with the Glorot initializer. The function you'll be calling is `tf.keras.initializers.GlorotNormal`, which draws samples from a truncated normal distribution centered on 0, with `stddev = sqrt(2 / (fan_in + fan_out))`, where `fan_in` is the number of input units and `fan_out` is the number of output units, both in the weight tensor. 

To initialize with zeros or ones you could use `tf.zeros()` or `tf.ones()` instead. 

<a name='ex-4'></a>
### Exercise 4 - initialize_parameters

Implement the function below to take in a shape and to return an array of numbers using the GlorotNormal initializer. 

 - `tf.keras.initializers.GlorotNormal(seed=1)`
 - `tf.Variable(initializer(shape=())`


<a name='2-4'></a> ### 2.4 - åˆå§‹åŒ–å‚æ•°
### 2.4 - åˆå§‹åŒ–å‚æ•° 

ç°åœ¨ï¼Œæ‚¨å°†ä½¿ç”¨Glorotåˆå§‹åŒ–å™¨åˆå§‹åŒ–ä¸€ä¸ªæ•°å­—å‘é‡ã€‚ä½ è¦è°ƒç”¨çš„å‡½æ•°æ˜¯`tf.keras.initializers.GlorotNormal`ï¼Œå®ƒä»ä»¥0ä¸ºä¸­å¿ƒçš„æˆªæ–­æ­£æ€åˆ†å¸ƒä¸­æŠ½å–æ ·æœ¬ï¼Œå…¶ä¸­`stddev = sqrt(2 / (fan_in + fan_out))`ï¼Œ
`fan_in`æ˜¯è¾“å…¥å•ä½çš„æ•°é‡ï¼Œ`fan_out`æ˜¯è¾“å‡ºå•ä½çš„æ•°é‡ï¼Œä¸¤è€…éƒ½åœ¨æƒé‡å¼ é‡ä¸­ã€‚

è¦ä½¿ç”¨0æˆ–1åˆå§‹åŒ–ï¼Œå¯ä»¥ä½¿ç”¨`tf.zeros()`æˆ–`tf.nes()`ä»£æ›¿ã€‚

<a name='ex-4'></a>.
### ç»ƒä¹  4 - åˆå§‹åŒ–å‚æ•°

å®ç°ä¸‹é¢çš„å‡½æ•°ï¼Œä½¿ç”¨GlorotNormalåˆå§‹åŒ–å™¨æ¥æ”¶ä¸€ä¸ªå½¢çŠ¶å¹¶è¿”å›ä¸€ä¸ªæ•°ç»„ã€‚

 - `tf.keras.initializers.GlorotNormal(seed=1)`ã€‚
 - `tf.Variable(initializer(shape=())`ã€‚

#### ä»€ä¹ˆæ˜¯GlorotNormal

Glorot initializerï¼Œä¹Ÿç§°ä¸ºXavieråˆå§‹åŒ–å™¨ï¼Œæ˜¯ä¸€ç§å¸¸ç”¨çš„æƒé‡åˆå§‹åŒ–æ–¹æ³•ï¼Œç”¨äºåˆå§‹åŒ–ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°ï¼ˆæƒé‡ï¼‰ã€‚å®ƒç”±Xavier Glorotå’ŒYoshua Bengioåœ¨2010å¹´æå‡ºï¼Œå¹¶è¢«å¹¿æ³›åº”ç”¨äºæ·±åº¦å­¦ä¹ ä¸­ã€‚

Glorotåˆå§‹åŒ–å™¨çš„ç›®æ ‡æ˜¯åœ¨ç½‘ç»œçš„ä¸åŒå±‚ä¹‹é—´ä¿æŒè¾“å…¥å’Œè¾“å‡ºçš„æ–¹å·®ç›¸ç­‰ã€‚å®ƒè€ƒè™‘äº†æ¯ä¸ªç¥ç»å…ƒçš„è¾“å…¥å’Œè¾“å‡ºè¿æ¥æ•°é‡ï¼Œä»¥åŠéçº¿æ€§æ¿€æ´»å‡½æ•°çš„ç‰¹æ€§ã€‚

å¯¹äºå…·æœ‰n_inä¸ªè¾“å…¥å’Œn_outä¸ªè¾“å‡ºçš„å±‚ï¼ŒGlorotåˆå§‹åŒ–å™¨ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•åˆå§‹åŒ–æƒé‡ï¼š

å¯¹äºå‡åŒ€åˆ†å¸ƒçš„æƒé‡åˆå§‹åŒ–ï¼ˆuniform distributionï¼‰ï¼Œæƒé‡åœ¨[-limit, limit]ä¹‹é—´å‡åŒ€é‡‡æ ·ï¼Œå…¶ä¸­limit = sqrt(6 / (n_in + n_out))ã€‚
å¯¹äºæ­£æ€åˆ†å¸ƒçš„æƒé‡åˆå§‹åŒ–ï¼ˆnormal distributionï¼‰ï¼Œæƒé‡ä»å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸ºsqrt(2 / (n_in + n_out))çš„æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·ã€‚
é€šè¿‡ä½¿ç”¨é€‚å½“çš„æ–¹å·®æ¥åˆå§‹åŒ–æƒé‡ï¼ŒGlorotåˆå§‹åŒ–å™¨æœ‰åŠ©äºé¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œä»è€Œæ›´å¥½åœ°è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œã€‚è¿™ç§åˆå§‹åŒ–æ–¹æ³•åœ¨è®¸å¤šå¸¸è§çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­éƒ½è¢«é»˜è®¤ä½¿ç”¨æˆ–ä½œä¸ºä¸€ç§é€‰æ‹©æä¾›ã€‚


**è¿™æ˜¯å¯¹æ¯ä¸€ä¸ªå·²ç»åˆå§‹åŒ–è¿‡çš„å‚æ•°ï¼Œè¿›è¡Œäº†glorotå¤„ç†**

# GRADED FUNCTION: initialize_parameters
```python
def initialize_parameters():
    """
    Initializes parameters to build a neural network with TensorFlow. The shapes are:
                        W1 : [25, 12288]
                        b1 : [25, 1]
                        W2 : [12, 25]
                        b2 : [12, 1]
                        W3 : [6, 12]
                        b3 : [6, 1]
    
    Returns:
    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3
    """
                                
    initializer = tf.keras.initializers.GlorotNormal(seed=1)   
    #(approx. 6 lines of code)
    # W1 = ...
    # b1 = ...
    # W2 = ...
    # b2 = ...
    # W3 = ...
    # b3 = ...
    # Initialize W1
    W1 = tf.Variable(initializer(shape=(25, 12288)))

    # Initialize b1
    b1 = tf.Variable(initializer((25, 1)))

    # Initialize W2
    W2 = tf.Variable(initializer(shape=(12, 25)))

    # Initialize b2
    b2 = tf.Variable(initializer(shape = (12, 1)))

    # Initialize W3
    W3 = tf.Variable(initializer(shape=(6, 12)))

    # Initialize b3
    b3 = tf.Variable(initializer((6, 1)))
    # YOUR CODE ENDS HERE

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2,
                  "W3": W3,
                  "b3": b3}
    
    return parameters
```
    
<a name='3'></a>
## 3 - Building Your First Neural Network in TensorFlow

In this part of the assignment you will build a neural network using TensorFlow. Remember that there are two parts to implementing a TensorFlow model:

- Implement forward propagation
- Retrieve the gradients and train the model

Let's get into it!

<a name='3-1'></a>
### 3.1 - Implement Forward Propagation 

One of TensorFlow's great strengths lies in the fact that you only need to implement the forward propagation function and it will keep track of the operations you did to calculate the back propagation automatically.  


<a name='ex-5'></a>
### Exercise 5 - forward_propagation

Implement the `forward_propagation` function.

**Note** Use only the TF API. 

- tf.math.add
- tf.linalg.matmul
- tf.keras.activations.relu

You will not apply "softmax" here. You'll see below, in `Exercise 6`, how the computation for it can be done internally by TensorFlow.



# GRADED FUNCTION: forward_propagation
```python
def forward_propagation(X, parameters):
    """
    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR
    
    Arguments:
    X -- input dataset placeholder, of shape (input size, number of examples)
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3"
                  the shapes are given in initialize_parameters

    Returns:
    Z3 -- the output of the last LINEAR unit
    """
    
    # Retrieve the parameters from the dictionary "parameters" 
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']
    
    #(approx. 5 lines)                   # Numpy Equivalents:
    # Z1 = ...                           # Z1 = np.dot(W1, X) + b1
    # A1 = ...                           # A1 = relu(Z1)
    # Z2 = ...                           # Z2 = np.dot(W2, A1) + b2
    # A2 = ...                           # A2 = relu(Z2)
    # Z3 = ...                           # Z3 = np.dot(W3, A2) + b3
    # YOUR CODE STARTS HERE
    Z1 = tf.linalg.matmul(W1, X) + b1
    A1 = tf.keras.activations.relu(Z1)
    Z2 = tf.linalg.matmul(W2, A1) + b2
    A2 = tf.keras.activations.relu(Z2)
    Z3 = tf.linalg.matmul(W3, A2) + b3
    # YOUR CODE ENDS HERE
    
    return Z3
```

<a name='3-2'></a>
### 3.2 Compute the Total Loss

All you have to do now is define the loss function that you're going to use. For this case, since we have a classification problem with 6 labels, a categorical cross entropy will work!

You are used to compute the cost value which sums the losses over the whole batch (i.e. all mini-batches) of samples, then divide the sum by the total number of samples. Here, you will achieve this in two steps. 

In step 1, the `compute_total_loss` function will only take care of summing the losses from one mini-batch of samples. Then, as you train the model (in section 3.3) which will call this `compute_total_loss` function once per mini-batch, step 2 will be done by accumulating the sums from each of the mini-batches, and finishing it with the division by the total number of samples to get the final cost value.

Computing the "total loss" instead of "mean loss" in step 1 can make sure the final cost value to be consistent. For example, if the mini-batch size is 4 but there are just 5 samples in the whole batch, then the last mini-batch is going to have 1 sample only. Considering the 5 samples, losses to be [0, 1, 2, 3, 4] respectively, we know the final cost should be their average which is 2. Adopting the "total loss" approach will get us the same answer. However, the "mean loss" approach will first get us 1.5 and 4 for the two mini-batches, and then finally 2.75 after taking average of them, which is different from the desired result of 2. Therefore, the "total loss" approach is adopted here. 

ç°åœ¨è¦åšçš„å°±æ˜¯å®šä¹‰è¦ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œç”±äºæˆ‘ä»¬é¢ä¸´çš„æ˜¯ä¸€ä¸ªæœ‰6ä¸ªæ ‡ç­¾çš„åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨åˆ†ç±»äº¤å‰ç†µï¼

æ‚¨éœ€è¦è®¡ç®—ä»£ä»·å€¼ï¼Œä»£ä»·å€¼ä¸ºæ•´æ‰¹æ ·æœ¬ï¼ˆå³æ‰€æœ‰å°æ‰¹é‡æ ·æœ¬ï¼‰çš„æŸå¤±æ€»å’Œï¼Œç„¶åç”¨æ€»å’Œé™¤ä»¥æ ·æœ¬æ€»æ•°ã€‚è¿™é‡Œåˆ†ä¸¤æ­¥å®ç°ã€‚

åœ¨ç¬¬ä¸€æ­¥ä¸­ï¼Œcompute_total_loss å‡½æ•°å°†åªè®¡ç®—ä¸€ä¸ªè¿·ä½ æ‰¹æ¬¡æ ·æœ¬çš„æŸå¤±æ€»å’Œã€‚ç„¶åï¼Œå½“æ‚¨è®­ç»ƒæ¨¡å‹æ—¶ï¼ˆåœ¨ç¬¬ 3.3 èŠ‚ä¸­ï¼‰ï¼Œæ¯ä¸ªè¿·ä½ æ‰¹æ¬¡å°†è°ƒç”¨ä¸€æ¬¡ compute_total_loss å‡½æ•°ï¼Œç¬¬ 2 æ­¥å°†é€šè¿‡ç´¯åŠ æ¯ä¸ªè¿·ä½ æ‰¹æ¬¡çš„æ€»å’Œæ¥å®Œæˆï¼Œæœ€åé™¤ä»¥æ ·æœ¬æ€»æ•°å¾—åˆ°æœ€ç»ˆçš„æˆæœ¬å€¼ã€‚

åœ¨æ­¥éª¤1ä¸­è®¡ç®— "æ€»æŸå¤± "è€Œä¸æ˜¯ "å¹³å‡æŸå¤± "å¯ä»¥ç¡®ä¿æœ€ç»ˆæˆæœ¬å€¼ä¿æŒä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå°æ‰¹é‡ä¸º 4 ä¸ªï¼Œä½†æ•´æ‰¹æ ·å“åªæœ‰ 5 ä¸ªï¼Œé‚£ä¹ˆæœ€åä¸€ä¸ªå°æ‰¹é‡åªæœ‰ 1 ä¸ªæ ·å“ã€‚è€ƒè™‘åˆ°è¿™5ä¸ªæ ·å“çš„æŸå¤±åˆ†åˆ«ä¸º[0, 1, 2, 3, 4]ï¼Œæˆ‘ä»¬çŸ¥é“æœ€ç»ˆæˆæœ¬åº”è¯¥æ˜¯å®ƒä»¬çš„å¹³å‡å€¼ï¼Œå³2ã€‚ä½†æ˜¯ï¼Œå¦‚æœé‡‡ç”¨ "å¹³å‡æŸå¤± "æ³•ï¼Œåˆ™ä¸¤ä¸ªå°æ‰¹é‡çš„æˆæœ¬åˆ†åˆ«ä¸º 1.5 å’Œ 4ï¼Œå–å…¶å¹³å‡å€¼åï¼Œæœ€ç»ˆæˆæœ¬ä¸º 2.75ï¼Œè¿™ä¸é¢„æœŸç»“æœ 2 ä¸åŒã€‚å› æ­¤ï¼Œè¿™é‡Œé‡‡ç”¨ "æ€»æŸå¤± "æ³•ã€‚

<a name='ex-6'></a>
### Exercise 6 -  compute_total_loss

Implement the total loss function below. You will use it to compute the total loss of a batch of samples. With this convenient function, you can sum the losses across many batches, and divide the sum by the total number of samples to get the cost value. 
- It's important to note that the "`y_pred`" and "`y_true`" inputs of [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) are expected to be of shape (number of examples, num_classes). 

- `tf.reduce_sum` does the summation over the examples.

- You skipped applying "softmax" in `Exercise 5` which will now be taken care by the `tf.keras.losses.categorical_crossentropy` by setting its parameter `from_logits=True` (You can read the response by one of our mentors [here](https://community.deeplearning.ai/t/week-3-assignment-compute-total-loss-try-to-set-from-logits-false/243049/2?u=paulinpaloalto) in the Community for the mathematical reasoning behind it. If you are not part of the Community already, you can do so by going [here](https://www.coursera.org/learn/deep-neural-network/ungradedLti/ZE1VR/important-have-questions-issues-or-ideas-join-our-community).)
  

ç»ƒä¹  6 - è®¡ç®—æ€»æŸå¤±
å®ç°ä¸‹é¢çš„æ€»æŸå¤±å‡½æ•°ã€‚æ‚¨å°†ç”¨å®ƒæ¥è®¡ç®—ä¸€æ‰¹æ ·æœ¬çš„æ€»æŸå¤±ã€‚æœ‰äº†è¿™ä¸ªæ–¹ä¾¿çš„å‡½æ•°ï¼Œæ‚¨å°±å¯ä»¥å°†è®¸å¤šæ‰¹æ¬¡çš„æŸå¤±ç›¸åŠ ï¼Œç„¶åå°†æ€»å’Œé™¤ä»¥æ ·æœ¬æ€»æ•°ï¼Œå¾—åˆ°æˆæœ¬å€¼ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œtf.keras.lossings.categorical_crossentropyçš„è¾“å…¥ "y_pred "å’Œ "y_true "é¢„è®¡ä¸ºå½¢çŠ¶ï¼ˆç¤ºä¾‹æ•°ã€ç±»æ•°ï¼‰ã€‚

tf.reduce_sumå°†å¯¹ç¤ºä¾‹æ±‚å’Œã€‚

æ‚¨åœ¨ç»ƒä¹  5 ä¸­è·³è¿‡äº†åº”ç”¨ "softmax"ï¼Œç°åœ¨å°†ç”± tf.keras.losses.categorical_crossentropy é€šè¿‡è®¾ç½®å…¶å‚æ•° from_logits=Trueï¼ˆæ‚¨å¯ä»¥é˜…è¯»æˆ‘ä»¬çš„ä¸€ä½å¯¼å¸ˆåœ¨ç¤¾åŒºä¸­çš„å›å¤ï¼Œäº†è§£å…¶èƒŒåçš„æ•°å­¦æ¨ç†ã€‚å¦‚æœæ‚¨è¿˜ä¸æ˜¯ç¤¾åŒºçš„ä¸€å‘˜ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»æ­¤å¤„åŠ å…¥)ã€‚




# GRADED FUNCTION: compute_total_loss 
```python
def compute_total_loss(logits, labels):
    """
    Computes the total loss
    
    Arguments:
    logits -- output of forward propagation (output of the last LINEAR unit), of shape (6, num_examples)
    labels -- "true" labels vector, same shape as Z3
    
    Returns:
    total_loss - Tensor of the total loss value
    """
    
    #(1 line of code)
    # remember to set `from_logits=True`
    # total_loss = ...
    # YOUR CODE STARTS HERE
    total_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True))
    
    # YOUR CODE ENDS HERE
    return total_loss

```

<a name='3-3'></a>
### 3.3 - Train the Model

Let's talk optimizers. You'll specify the type of optimizer in one line, in this case `tf.keras.optimizers.Adam` (though you can use others such as SGD), and then call it within the training loop. 

Notice the `tape.gradient` function: this allows you to retrieve the operations recorded for automatic differentiation inside the `GradientTape` block. Then, calling the optimizer method `apply_gradients`, will apply the optimizer's update rules to each trainable parameter. At the end of this assignment, you'll find some documentation that explains this more in detail, but for now, a simple explanation will do. ;) 


Here you should take note of an important extra step that's been added to the batch training process: 

- `tf.Data.dataset = dataset.prefetch(8)` 

What this does is prevent a memory bottleneck that can occur when reading from disk. `prefetch()` sets aside some data and keeps it ready for when it's needed. It does this by creating a source dataset from your input data, applying a transformation to preprocess the data, then iterating over the dataset the specified number of elements at a time. This works because the iteration is streaming, so the data doesn't need to fit into the memory. 

3.2 è®¡ç®—æ€»æŸå¤±
ç°åœ¨è¦åšçš„å°±æ˜¯å®šä¹‰è¦ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œç”±äºæˆ‘ä»¬é¢ä¸´çš„æ˜¯ä¸€ä¸ªæœ‰6ä¸ªæ ‡ç­¾çš„åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨åˆ†ç±»äº¤å‰ç†µï¼

æ‚¨éœ€è¦è®¡ç®—ä»£ä»·å€¼ï¼Œä»£ä»·å€¼ä¸ºæ•´æ‰¹æ ·æœ¬ï¼ˆå³æ‰€æœ‰å°æ‰¹é‡æ ·æœ¬ï¼‰çš„æŸå¤±æ€»å’Œï¼Œç„¶åç”¨æ€»å’Œé™¤ä»¥æ ·æœ¬æ€»æ•°ã€‚è¿™é‡Œåˆ†ä¸¤æ­¥å®ç°ã€‚

åœ¨ç¬¬ä¸€æ­¥ä¸­ï¼Œcompute_total_loss å‡½æ•°å°†åªè®¡ç®—ä¸€ä¸ªè¿·ä½ æ‰¹æ¬¡æ ·æœ¬çš„æŸå¤±æ€»å’Œã€‚ç„¶åï¼Œå½“æ‚¨è®­ç»ƒæ¨¡å‹æ—¶ï¼ˆåœ¨ç¬¬ 3.3 èŠ‚ä¸­ï¼‰ï¼Œæ¯ä¸ªè¿·ä½ æ‰¹æ¬¡å°†è°ƒç”¨ä¸€æ¬¡ compute_total_loss å‡½æ•°ï¼Œç¬¬ 2 æ­¥å°†é€šè¿‡ç´¯åŠ æ¯ä¸ªè¿·ä½ æ‰¹æ¬¡çš„æ€»å’Œæ¥å®Œæˆï¼Œæœ€åé™¤ä»¥æ ·æœ¬æ€»æ•°å¾—åˆ°æœ€ç»ˆçš„æˆæœ¬å€¼ã€‚

åœ¨æ­¥éª¤1ä¸­è®¡ç®— "æ€»æŸå¤± "è€Œä¸æ˜¯ "å¹³å‡æŸå¤± "å¯ä»¥ç¡®ä¿æœ€ç»ˆæˆæœ¬å€¼ä¿æŒä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå°æ‰¹é‡ä¸º 4 ä¸ªï¼Œä½†æ•´æ‰¹æ ·å“åªæœ‰ 5 ä¸ªï¼Œé‚£ä¹ˆæœ€åä¸€ä¸ªå°æ‰¹é‡åªæœ‰ 1 ä¸ªæ ·å“ã€‚è€ƒè™‘åˆ°è¿™5ä¸ªæ ·å“çš„æŸå¤±åˆ†åˆ«ä¸º[0, 1, 2, 3, 4]ï¼Œæˆ‘ä»¬çŸ¥é“æœ€ç»ˆæˆæœ¬åº”è¯¥æ˜¯å®ƒä»¬çš„å¹³å‡å€¼ï¼Œå³2ã€‚ç„¶è€Œï¼Œé‡‡ç”¨ "å¹³å‡æŸå¤± "çš„æ–¹æ³•å°†é¦–å…ˆå¾—åˆ°ä¸¤ä¸ªå°æ‰¹é‡çš„ 1.5 å’Œ 4ï¼Œç„¶ååœ¨æ±‚å‡ºå®ƒä»¬çš„å¹³å‡å€¼åå¾—åˆ° 2.75ï¼Œè¿™ä¸æœŸæœ›çš„ç»“æœ 2 ä¸åŒã€‚å› æ­¤ï¼Œè¿™é‡Œé‡‡ç”¨ "æ€»æŸå¤± "çš„æ–¹æ³•ã€‚


ç»ƒä¹  6 - è®¡ç®—æ€»æŸå¤±
å®ç°ä¸‹é¢çš„æ€»æŸå¤±å‡½æ•°ã€‚æ‚¨å°†ç”¨å®ƒæ¥è®¡ç®—ä¸€æ‰¹æ ·æœ¬çš„æ€»æŸå¤±ã€‚æœ‰äº†è¿™ä¸ªæ–¹ä¾¿çš„å‡½æ•°ï¼Œæ‚¨å°±å¯ä»¥å°†è®¸å¤šæ‰¹æ¬¡çš„æŸå¤±ç›¸åŠ ï¼Œç„¶åå°†æ€»å’Œé™¤ä»¥æ ·æœ¬æ€»æ•°ï¼Œå¾—åˆ°æˆæœ¬å€¼ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œtf.keras.lossings.categorical_crossentropyçš„è¾“å…¥ "y_pred "å’Œ "y_true "é¢„è®¡ä¸ºå½¢çŠ¶ï¼ˆç¤ºä¾‹æ•°ã€ç±»æ•°ï¼‰ã€‚

tf.reduce_sumå°†å¯¹ç¤ºä¾‹æ±‚å’Œã€‚

æ‚¨åœ¨ç»ƒä¹  5 ä¸­è·³è¿‡äº†åº”ç”¨ "softmax"ï¼Œç°åœ¨å°†ç”± tf.keras.losses.categorical_crossentropy é€šè¿‡è®¾ç½®å…¶å‚æ•° from_logits=Trueï¼ˆæ‚¨å¯ä»¥é˜…è¯»æˆ‘ä»¬çš„ä¸€ä½å¯¼å¸ˆåœ¨ç¤¾åŒºä¸­çš„å›å¤ï¼Œäº†è§£å…¶èƒŒåçš„æ•°å­¦æ¨ç†ã€‚å¦‚æœæ‚¨è¿˜ä¸æ˜¯ç¤¾åŒºçš„ä¸€å‘˜ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»æ­¤å¤„åŠ å…¥)ã€‚


**Disputed results**

# GRADED FUNCTION: compute_total_loss 
```python
def compute_total_loss(logits, labels):
    """
    Computes the total loss
    
    Arguments:
    logits -- output of forward propagation (output of the last LINEAR unit), of shape (6, num_examples)
    labels -- "true" labels vector, same shape as Z3
    
    Returns:
    total_loss - Tensor of the total loss value
    """
    
    #(1 line of code)
    # remember to set `from_logits=True`
    # total_loss = ...
    # YOUR CODE STARTS HERE
    total_loss = tf.reduce_sum(tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True))

    # YOUR CODE ENDS HERE
    return total_loss
```



<a name='3-3'></a>
### 3.3 - Train the Model

Let's talk optimizers. You'll specify the type of optimizer in one line, in this case `tf.keras.optimizers.Adam` (though you can use others such as SGD), and then call it within the training loop. 

Notice the `tape.gradient` function: this allows you to retrieve the operations recorded for automatic differentiation inside the `GradientTape` block. Then, calling the optimizer method `apply_gradients`, will apply the optimizer's update rules to each trainable parameter. At the end of this assignment, you'll find some documentation that explains this more in detail, but for now, a simple explanation will do. ;) 


Here you should take note of an important extra step that's been added to the batch training process: 

- `tf.Data.dataset = dataset.prefetch(8)` 

What this does is prevent a memory bottleneck that can occur when reading from disk. `prefetch()` sets aside some data and keeps it ready for when it's needed. It does this by creating a source dataset from your input data, applying a transformation to preprocess the data, then iterating over the dataset the specified number of elements at a time. This works because the iteration is streaming, so the data doesn't need to fit into the memory. 

```python
def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,
          num_epochs = 1500, minibatch_size = 32, print_cost = True):
    """
    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.
    
    Arguments:
    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)
    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)
    X_test -- training set, of shape (input size = 12288, number of training examples = 120)
    Y_test -- test set, of shape (output size = 6, number of test examples = 120)
    learning_rate -- learning rate of the optimization
    num_epochs -- number of epochs of the optimization loop
    minibatch_size -- size of a minibatch
    print_cost -- True to print the cost every 10 epochs
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """
    
    costs = []                                        # To keep track of the cost
    train_acc = []
    test_acc = []
    
    # Initialize your parameters
    #(1 line)
    parameters = initialize_parameters()

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']

    optimizer = tf.keras.optimizers.Adam(learning_rate)
    
    # The CategoricalAccuracy will track the accuracy for this multiclass problem
    test_accuracy = tf.keras.metrics.CategoricalAccuracy()
    train_accuracy = tf.keras.metrics.CategoricalAccuracy()
    
    dataset = tf.data.Dataset.zip((X_train, Y_train))
    test_dataset = tf.data.Dataset.zip((X_test, Y_test))
    
    # We can get the number of elements of a dataset using the cardinality method
    m = dataset.cardinality().numpy()
    
    minibatches = dataset.batch(minibatch_size).prefetch(8)
    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)
    #X_train = X_train.batch(minibatch_size, drop_remainder=True).prefetch(8)# <<< extra step    
    #Y_train = Y_train.batch(minibatch_size, drop_remainder=True).prefetch(8) # loads memory faster 

    # Do the training loop
    for epoch in range(num_epochs):

        epoch_total_loss = 0.
        
        #We need to reset object to start measuring from 0 the accuracy each epoch
        train_accuracy.reset_states()
        
        for (minibatch_X, minibatch_Y) in minibatches:
            
            with tf.GradientTape() as tape:
                # 1. predict
                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)

                # 2. loss
                minibatch_total_loss = compute_total_loss(Z3, tf.transpose(minibatch_Y))

            # We accumulate the accuracy of all the batches
            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))
            
            trainable_variables = [W1, b1, W2, b2, W3, b3]
            grads = tape.gradient(minibatch_total_loss, trainable_variables)
            optimizer.apply_gradients(zip(grads, trainable_variables))
            epoch_total_loss += minibatch_total_loss
        
        # We divide the epoch total loss over the number of samples
        epoch_total_loss /= m

        # Print the cost every 10 epochs
        if print_cost == True and epoch % 10 == 0:
            print ("Cost after epoch %i: %f" % (epoch, epoch_total_loss))
            print("Train accuracy:", train_accuracy.result())
            
            # We evaluate the test set every 10 epochs to avoid computational overhead
            for (minibatch_X, minibatch_Y) in test_minibatches:
                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)
                test_accuracy.update_state(minibatch_Y, tf.transpose(Z3))
            print("Test_accuracy:", test_accuracy.result())

            costs.append(epoch_total_loss)
            train_acc.append(train_accuracy.result())
            test_acc.append(test_accuracy.result())
            test_accuracy.reset_states()


    return parameters, costs, train_acc, test_acc
```
