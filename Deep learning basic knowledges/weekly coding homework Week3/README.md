 ### å®ç°ä¸€ä¸ªå…·æœ‰å•ä¸€éšè—å±‚çš„2ç±»åˆ†ç±»ç¥ç»ç½‘ç»œ###

- ä½¿ç”¨å…·æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°çš„å•å…ƒï¼Œå¦‚tanh
- è®¡ç®—äº¤å‰ç†µæŸå¤±
- å®ç°å‰å‘å’Œåå‘ä¼ æ’­

- <a name='1'></a>
# 1 - Packages

First import all the packages that you will need during this assignment.

- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.
- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. 
- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python.
- testCases provides some test examples to assess the correctness of your functions
- planar_utils provide various useful functions used in this assignment

  é¦–å…ˆå¯¼å…¥æ‰€æœ‰ä½ åœ¨è¿™æ¬¡ä½œä¸šä¸­éœ€è¦çš„åŒ…ã€‚

- numpyæ˜¯ç”¨Pythonè¿›è¡Œç§‘å­¦è®¡ç®—çš„åŸºæœ¬åŒ…ã€‚
- sklearnä¸ºæ•°æ®æŒ–æ˜å’Œæ•°æ®åˆ†ææä¾›äº†ç®€å•è€Œæœ‰æ•ˆçš„å·¥å…·ã€‚
- matplotlibæ˜¯ä¸€ä¸ªç”¨äºåœ¨Pythonä¸­ç»˜åˆ¶å›¾å½¢çš„åº“ã€‚
- testCases æä¾›äº†ä¸€äº›æµ‹è¯•å®ä¾‹ï¼Œä»¥è¯„ä¼°ä½ çš„å‡½æ•°çš„æ­£ç¡®æ€§ã€‚
- planar_utilsæä¾›äº†æœ¬ä½œä¸šä¸­ä½¿ç”¨çš„å„ç§æœ‰ç”¨çš„å‡½æ•°


  **Coding**

        # Package imports
        import numpy as np
        import copy
        import matplotlib.pyplot as plt
        from testCases_v2 import *
        from public_tests import *
        import sklearn
        import sklearn.datasets
        import sklearn.linear_model
        from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

        %matplotlib inline

        %load_ext autoreload
        %autoreload 2

  <a name='2'></a>
# 2 - Load the Dataset 

        X, Y = load_planar_dataset()

ä½¿ç”¨matplotlibå¯¹æ•°æ®é›†è¿›è¡Œå¯è§†åŒ–ã€‚è¯¥æ•°æ®çœ‹èµ·æ¥åƒä¸€æœµ "èŠ±"ï¼Œæœ‰ä¸€äº›çº¢è‰²ï¼ˆæ ‡ç­¾y=0ï¼‰å’Œä¸€äº›è“è‰²ï¼ˆy=1ï¼‰çš„ç‚¹ã€‚ä½ çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªæ¨¡å‹æ¥é€‚åº”è¿™ä¸ªæ•°æ®ã€‚
æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ†ç±»å™¨èƒ½å°†åŒºåŸŸå®šä¹‰ä¸ºçº¢è‰²æˆ–è“è‰²ã€‚

![11](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/d987eef8-ce10-478c-b68d-e768e25673da)

![12](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/5c910039-0b9e-4964-aa43-995ec9941fcf)

You have:

- a numpy-array (matrix) X that contains your features (x1, x2)
- a numpy-array (vector) Y that contains your labels (red:0, blue:1).
First, get a better sense of what your data is like.

### åˆ¤æ–­Xï¼ˆç‰¹å¾ï¼‰å’ŒY(æ ‡ç­¾)çš„æ•°é‡æœ‰å¤šå°‘

         # (â‰ˆ 3 lines of code)
         # shape_X = ...
         # shape_Y = ...
         # training set size
         # m = ...
         # YOUR CODE STARTS HERE
         shape_X = X.shape
         shape_Y = Y.shape
         m = X.shape[1]
         # YOUR CODE ENDS HERE

         print ('The shape of X is: ' + str(shape_X))
         print ('The shape of Y is: ' + str(shape_Y))
         print ('I have m = %d training examples!' % (m))

![13](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/5e8b8865-ba73-45a1-b3de-b83cc5ea7932)


### 3 - ç®€å•é€»è¾‘å›å½’
åœ¨æ„å»ºå®Œæ•´çš„ç¥ç»ç½‘ç»œä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆçœ‹çœ‹é€»è¾‘å›å½’å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ sklearn çš„å†…ç½®å‡½æ•°æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚è¿è¡Œä¸‹é¢çš„ä»£ç ä»¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨ã€‚

![14](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/d3c48807-7595-4bc1-b7de-d4243a2d2bbc)

![15](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/5a43732d-3c9a-4949-bd7a-bb7b63f28f75)

<a name='4'></a>
## 4 - Neural Network model

Logistic regression didn't work well on the flower dataset. Next, you're going to train a Neural Network with a single hidden layer and see how that handles the same problem.

**The model**:
![16](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/1f6225ac-b4c1-4984-b696-675bf59793a6)



**Reminder**: The general methodology to build a Neural Network is to:
    1. Define the neural network structure ( # of input units,  # of hidden units, etc). 
    2. Initialize the model's parameters
    3. Loop:
        - Implement forward propagation
        - Compute loss
        - Implement backward propagation to get the gradients
        - Update parameters (gradient descent)


**æé†’**ï¼š æ„å»ºç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ–¹æ³•æ˜¯ï¼šï¼š
    1. å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„ï¼ˆè¾“å…¥å•å…ƒçš„æ•°é‡ï¼Œéšè—å•å…ƒçš„æ•°é‡ç­‰ï¼‰ã€‚
    2. åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
    3. å¾ªç¯ï¼š
        - å®æ–½å‰å‘ä¼ æ’­
        - è®¡ç®—æŸå¤±
        - å®æ–½åå‘ä¼ æ’­ä»¥è·å¾—æ¢¯åº¦
        - æ›´æ–°å‚æ•°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰

In practice, you'll often build helper functions to compute steps 1-3, then merge them into one function called `nn_model()`. Once you've built `nn_model()` and learned the right parameters, you can make predictions on new data.


åœ¨å®è·µä¸­ï¼Œä½ é€šå¸¸ä¼šå»ºç«‹è¾…åŠ©å‡½æ•°æ¥è®¡ç®—ç¬¬1-3æ­¥ï¼Œç„¶åå°†å®ƒä»¬åˆå¹¶åˆ°ä¸€ä¸ªå«åš`nn_model()`çš„å‡½æ•°ä¸­ã€‚ä¸€æ—¦ä½ å»ºç«‹äº†`nn_model()`å¹¶å­¦ä¹ äº†æ­£ç¡®çš„å‚æ•°ï¼Œä½ å°±å¯ä»¥å¯¹æ–°çš„æ•°æ®è¿›è¡Œé¢„æµ‹äº†ã€‚


<a name='4-1'></a>
### 4.1 - Defining the neural network structure ####

<a name='ex-2'></a>
### Exercise 2 - layer_sizes 

Define three variables:
    - n_x: the size of the input layer
    - n_h: the size of the hidden layer (**set this to 4, only for this Exercise 2**) 
    - n_y: the size of the output layer

**Hint**: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.

4.1 - å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„

ç»ƒä¹ 2--å±‚_å°ºå¯¸
å®šä¹‰ä¸‰ä¸ªå˜é‡ï¼š

- n_x: è¾“å…¥å±‚çš„å¤§å°
- n_hï¼šéšè—å±‚çš„å¤§å°ï¼ˆ**å°†å…¶è®¾ä¸º4ï¼Œä»…ç”¨äºæœ¬ç»ƒä¹ 2**ï¼‰ã€‚
- n_yï¼šè¾“å‡ºå±‚çš„å¤§å°ã€‚
- 
æç¤ºï¼šä½¿ç”¨Xå’ŒYçš„å½¢çŠ¶æ¥å¯»æ‰¾n_xå’Œn_yã€‚åŒæ—¶ï¼Œå°†éšè—å±‚çš„å¤§å°ç¡¬ç¼–ç ä¸º4ã€‚

åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¾“å…¥å±‚çš„ç¥ç»å…ƒæ•°é‡ç”±ç‰¹å¾æ•°æ®é›†çš„ç»´åº¦å†³å®šã€‚æ¯ä¸ªç‰¹å¾åœ¨ç¥ç»ç½‘ç»œä¸­å¯¹åº”ä¸€ä¸ªè¾“å…¥ç¥ç»å…ƒã€‚ä¾‹å¦‚ï¼Œå¦‚æœç‰¹å¾æ•°æ®é›†Xçš„å½¢çŠ¶ä¸º(n_x, m)ï¼Œå…¶ä¸­n_xè¡¨ç¤ºç‰¹å¾çš„æ•°é‡ï¼Œ
mè¡¨ç¤ºæ ·æœ¬çš„æ•°é‡ï¼Œé‚£ä¹ˆè¾“å…¥å±‚çš„ç¥ç»å…ƒæ•°é‡å°±æ˜¯n_xã€‚

è¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°é‡é€šå¸¸ç”±ä»»åŠ¡çš„è¦æ±‚å†³å®šã€‚åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œè¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°é‡é€šå¸¸ä¸ç±»åˆ«çš„æ•°é‡ç›¸åŒï¼Œæ¯ä¸ªè¾“å‡ºç¥ç»å…ƒå¯¹åº”ä¸€ä¸ªç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œå¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºå±‚æœ‰ä¸¤ä¸ªç¥ç»å…ƒï¼Œ
åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªç±»åˆ«ã€‚å¯¹äºå¤šåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°é‡ç­‰äºç±»åˆ«çš„æ•°é‡ã€‚

å› æ­¤ï¼Œè¾“å…¥å±‚å’Œè¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°é‡æ˜¯æ ¹æ®é—®é¢˜çš„ç‰¹æ€§å’Œè¦æ±‚æ¥ç¡®å®šçš„ï¼Œè€Œéšè—å±‚çš„ç¥ç»å…ƒæ•°é‡å¯ä»¥æ ¹æ®ç½‘ç»œæ¶æ„å’Œå®éªŒéœ€æ±‚è¿›è¡Œè®¾ç½®ã€‚

**Code**

# GRADED FUNCTION: layer_sizes

        def layer_sizes(X, Y):
    """
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    """
    #(â‰ˆ 3 lines of code)
    # n_x = ... 
    # n_h = ...
    # n_y = ... 
    # YOUR CODE STARTS HERE
    n_x = X.shape[0]
    n_h = 4
    n_y = Y.shape[0]
    
    # YOUR CODE ENDS HERE
    return (n_x, n_h, n_y)

     # è·å–layer_sizesçš„æµ‹è¯•æ¡ˆä¾‹ï¼Œ è¿™ä¸ªæ¡ˆä¾‹ä¸­çš„æ•°æ®å’Œä¹‹å‰çš„é‚£ä¸ªæ— å…³
     t_X, t_Y = layer_sizes_test_case()

     # è®¡ç®—å±‚çš„å¤§å°
     (n_x, n_h, n_y) = layer_sizes(t_X, t_Y)

     # æ‰“å°è¾“å…¥å±‚çš„å¤§å°
     print("è¾“å…¥å±‚çš„å¤§å°ä¸ºï¼šn_x = " + str(n_x))

     # æ‰“å°éšè—å±‚çš„å¤§å°
     print("éšè—å±‚çš„å¤§å°ä¸ºï¼šn_h = " + str(n_h))

     # æ‰“å°è¾“å‡ºå±‚çš„å¤§å°
     print("è¾“å‡ºå±‚çš„å¤§å°ä¸ºï¼šn_y = " + str(n_y))

     # è¿è¡Œlayer_sizesçš„æµ‹è¯•
     layer_sizes_test(layer_sizes)

![17](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/6fb1cf5e-6790-44bb-8c29-6a7e1d9f8b46)


### åœ¨ç¡®å®šå®Œshallowç¥ç»ç½‘ç»œçš„structureä¹‹åï¼Œå°±æ˜¯å¯¹æ•°æ®è¿›è¡Œinitializingã€‚

ä¸€èˆ¬ä½¿ç”¨éšæœºè¿˜åŸæ³•ï¼Œnp.random.randn(a,b) * 0.01ï¼Œä¸é€‚ç”¨å½’0æ³•æ˜¯å› ä¸ºï¼Œå¦‚æœhidden layeréƒ½æ˜¯å½’0åˆ™ï¼Œéšè—å±‚æ²¡æ„ä¹‰äº†.
<a name='4-2'></a>
### 4.2 - Initialize the model's parameters ####

<a name='ex-3'></a>
### Exercise 3 -  initialize_parameters

Implement the function `initialize_parameters()`.

**Instructions**:
- Make sure your parameters' sizes are right. Refer to the neural network figure above if needed.
- You will initialize the weights matrices with random values. 
    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).
- You will initialize the bias vectors as zeros. 
    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros.
 
**ä»£ç è§£é‡Š**


W1 = np.random.randn(n_h, n_x) * 0.01 
**å…¶ä¸­ï¼Œn_hæ˜¯è¿™ä¸ªarrayçš„è¡Œï¼Œè¡¨ç¤ºçš„æ˜¯ç‰¹å¾ï¼Œæ‰€ä»¥è¡¨ç¤ºè¿™ä¸ªlayerç¥ç»å…ƒçš„æ•°é‡ï¼Œè€Œn_xè¡¨ç¤ºçš„æ˜¯åˆ—ï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸€ä¸ªå±‚çº§inputåˆ°è¿™ä¸ªå±‚çº§çš„æ•°é‡**
  
b1 = np.zeros((n_h,1))
**bæ˜¯ä¸€ä¸ªå¸¸æ•°åˆ—ï¼Œä»–æ˜¯ä¸å—ä¸Šä¸€å±‚çº§å½±åƒçš„ï¼Œæ‰€ä»¥åé¢æ˜¯1**
W2 = np.random.randn(n_y, n_h) * 0.01
 b2 = np.zeros((n_y,1))


        # GRADED FUNCTION: initialize_parameters

        def initialize_parameters(n_x, n_h, n_y):
            """
            Argument:
            n_x -- size of the input layer
            n_h -- size of the hidden layer
            n_y -- size of the output layer
    
            Returns:
            params -- python dictionary containing your parameters:
                            W1 -- weight matrix of shape (n_h, n_x)
                            b1 -- bias vector of shape (n_h, 1)
                            W2 -- weight matrix of shape (n_y, n_h)
                            b2 -- bias vector of shape (n_y, 1)
            """    
            #(â‰ˆ 4 lines of code)
            # W1 = ...
            # b1 = ...
            # W2 = ...
            # b2 = ...
            # YOUR CODE STARTS HERE
            W1 = np.random.randn(n_h, n_x) * 0.01
            b1 = np.zeros((n_h,1))
            W2 = np.random.randn(n_y, n_h) * 0.01
            b2 = np.zeros((n_y,1))
    
            # YOUR CODE ENDS HERE

            parameters = {"W1": W1,
                          "b1": b1,
                          "W2": W2,
                          "b2": b2}
    
            return parameters

     
![18](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/12dba306-5c22-457e-830b-fe96df0940b0)


åœ¨å®šä¹‰å®Œå±‚çº§ï¼Œ
å®šä¹‰å®Œinitailizeå‡½æ•°ä¹‹åï¼Œå°±å¯ä»¥å®šä¹‰forward propagate å‡½æ•°äº†.

<a name='4-3'></a>
### 4.3 - The Loop 

<a name='ex-4'></a>
### Exercise 4 - forward_propagation

Implement `forward_propagation()` using the following equations:

$$Z^{[1]} =  W^{[1]} X + b^{[1]}\tag{1}$$ 
$$A^{[1]} = \tanh(Z^{[1]})\tag{2}$$
$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\tag{3}$$
$$\hat{Y} = A^{[2]} = \sigma(Z^{[2]})\tag{4}$$


**Instructions**:

- Check the mathematical representation of your classifier in the figure above.
- Use the function `sigmoid()`. It's built into (imported) this notebook.
- Use the function `np.tanh()`. It's part of the numpy library.
- Implement using these steps:
    1. Retrieve each parameter from the dictionary "parameters" (which is the output of `initialize_parameters()` by using `parameters[".."]`.
    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).
- Values needed in the backpropagation are stored in "cache". The cache will be given as an input to the backpropagation function.

è¿™æ¬¡çš„hidden layerä¸­ä½¿ç”¨çš„æ˜¯ï¼Œnumpy libraryä¸­çš„np.tanh()å‡½æ•°ï¼Œå¹¶ä¸”tanh as the active function for hidden layer.


**Coding**

        # GRADED FUNCTION:forward_propagation

        def forward_propagation(X, parameters):
            """
            Argument:
            X -- input data of size (n_x, m)
            parameters -- python dictionary containing your parameters (output of initialization function)
    
            Returns:
            A2 -- The sigmoid output of the second activation
            cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
            """
            # Retrieve each parameter from the dictionary "parameters"
            #(â‰ˆ 4 lines of code)
            # W1 = ...
            # b1 = ...
            # W2 = ...
            # b2 = ...
            # YOUR CODE STARTS HERE
            W1 = parameters["W1"]
            b1 = parameters["b1"]
            W2 = parameters["W2"]
            b2 = parameters["b2"]
    
            # YOUR CODE ENDS HERE
    
            # Implement Forward Propagation to calculate A2 (probabilities)
            # (â‰ˆ 4 lines of code)
            # Z1 = ...
            # A1 = ...
            # Z2 = ...
            # A2 = ...
            # YOUR CODE STARTS HERE
            Z1 = np.dot(W1,X) + b1
            A1 = np.tanh(Z1)
            Z2 = np.dot(W2,A1) + b2
            A2 = 1/(1+np.exp(-Z2))
    
            # YOUR CODE ENDS HERE
    
            assert(A2.shape == (1, X.shape[1]))
    
            cache = {"Z1": Z1,
                     "A1": A1,
                     "Z2": Z2,
                     "A2": A2}
    
            return A2, cache

**ä»¥ä¸‹æ˜¯ä»æµ‹è¯•æ•°é›†ä¸­æå–çš„**

![19](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/11dbc883-2b95-4071-85f0-751a2dec57d9)

<a name='4-4'></a>
### 4.4 - Compute the Cost

Now that you've computed $A^{[2]}$ (in the Python variable "`A2`"), which contains $a^{[2](i)}$ for all examples, you can compute the cost function as follows:

$$J = - \frac{1}{m} \sum\limits_{i = 1}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}$$

<a name='ex-5'></a>
### Exercise 5 - compute_cost 

Implement `compute_cost()` to compute the value of the cost $J$.

**Instructions**:
- There are many ways to implement the cross-entropy loss. This is one way to implement one part of the equation without for loops:  âˆ’âˆ‘ğ‘–=1ğ‘šğ‘¦(ğ‘–)log(ğ‘[2](ğ‘–)) :
  
```python
logprobs = np.multiply(np.log(A2),Y)
cost = - np.sum(logprobs)          
```

- Use that to build the whole expression of the cost function.

**Notes**: 

- You can use either `np.multiply()` and then `np.sum()` or directly `np.dot()`).  
- If you use `np.multiply` followed by `np.sum` the end result will be a type `float`, whereas if you use `np.dot`, the result will be a 2D numpy array.  
- You can use `np.squeeze()` to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array). 
- You can also cast the array as a type `float` using `float()`.


- ä½ å¯ä»¥ä½¿ç”¨np.multiply()ç„¶ånp.sum()æˆ–è€…ç›´æ¥ä½¿ç”¨np.dot())ã€‚
- å¦‚æœä½ ä½¿ç”¨np.multiplyï¼Œç„¶åå†ä½¿ç”¨np.sumï¼Œæœ€ç»ˆçš„ç»“æœå°†æ˜¯ä¸€ä¸ªæµ®ç‚¹ç±»å‹ï¼Œè€Œå¦‚æœä½ ä½¿ç”¨np.dotï¼Œç»“æœå°†æ˜¯ä¸€ä¸ª2Dçš„numpyæ•°ç»„ã€‚
- ä½ å¯ä»¥ä½¿ç”¨np.squeeze()æ¥å»é™¤å¤šä½™çš„ç»´åº¦ï¼ˆå¦‚æœæ˜¯å•ä¸€çš„floatï¼Œè¿™å°†è¢«å‡å°‘ä¸ºä¸€ä¸ªé›¶ç»´æ•°ç»„ï¼‰ã€‚
- ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨float()å°†æ•°ç»„è½¬æ¢ä¸ºfloatç±»å‹ã€‚

  ä»£ç è§£é‡Šï¼š

logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y): è¿™è¡Œä»£ç è®¡ç®—äº†A2çš„å¯¹æ•°ä¸Yçš„å…ƒç´ çº§ä¹˜ç§¯ï¼Œä»¥åŠ(1 - A2)çš„å¯¹æ•°ä¸(1 - Y)çš„å…ƒç´ çº§ä¹˜ç§¯ã€‚å®ƒè®¡ç®—äº†é¢„æµ‹å€¼ï¼ˆA2ï¼‰å’ŒçœŸå®å€¼ï¼ˆYï¼‰çš„å¯¹æ•°æ¦‚ç‡ã€‚

cost = - np.sum(logprobs) / m: è¿™è¡Œä»£ç é€šè¿‡å¯¹æ‰€æœ‰å¯¹æ•°æ¦‚ç‡è¿›è¡Œæ±‚å’Œå¹¶é™¤ä»¥ç¤ºä¾‹æ•°é‡mï¼Œè®¡ç®—äº†å¹³å‡äº¤å‰ç†µæˆæœ¬ã€‚è´Ÿå·ç”¨äºç¿»è½¬æ±‚å’Œçš„ç¬¦å·ï¼Œå› ä¸ºäº¤å‰ç†µæˆæœ¬åœ¨æ–¹ç¨‹ä¸­å®šä¹‰ä¸ºè´Ÿæ•°ã€‚

cost = float(np.squeeze(cost)): è¿™è¡Œä»£ç é€šè¿‡å‹ç¼©æ“ä½œå°†å½¢çŠ¶ä¸ºï¼ˆ1ï¼Œ1ï¼‰çš„äºŒç»´æ•°ç»„çš„æˆæœ¬è½¬æ¢ä¸ºæ ‡é‡å€¼ã€‚np.squeeze()å‡½æ•°ä¼šåˆ é™¤å¤§å°ä¸º1çš„ä»»ä½•ç»´åº¦ï¼Œæ‰€ä»¥å®ƒå°†[[17]]è½¬æ¢ä¸º17ï¼ˆæ ‡é‡å€¼ï¼‰ã€‚

æœ€åï¼Œcostå˜é‡ä½œä¸ºå‡½æ•°çš„è¾“å‡ºè¿”å›ã€‚

æ€»ä½“è€Œè¨€ï¼Œcompute_costå‡½æ•°è®¡ç®—äº†é¢„æµ‹å€¼ï¼ˆA2ï¼‰å’ŒçœŸå®å€¼ï¼ˆYï¼‰ä¹‹é—´çš„äº¤å‰ç†µæˆæœ¬ã€‚è¿™ä¸ªæˆæœ¬ç”¨äºè¯„ä¼°ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½ã€‚

**Coding**

        # GRADED FUNCTION: compute_cost

       def compute_cost(A2, Y):
           """
           Computes the cross-entropy cost given in equation (13)
    
           Arguments:
           A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
           Y -- "true" labels vector of shape (1, number of examples)

           Returns:
           cost -- cross-entropy cost given equation (13)
    
           """
    
           m = Y.shape[1] # number of examples

           # Compute the cross-entropy cost
           # (â‰ˆ 2 lines of code)
           # logprobs = ...
           # cost = ...
           # YOUR CODE STARTS HERE
           logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)
           cost = - np.sum(logprobs) / m
    
           # YOUR CODE ENDS HERE
    
           cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. 
                                           # E.g., turns [[17]] into 17 
           
           return cost

**ä»£ç æµ‹è¯•è¾“å‡ºï¼Œç»“æœæ˜¯ä»æµ‹è¯•æ–‡ä»¶ä¸­æ¥çš„**

![20](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/87196a98-196a-4965-801d-9d567c30a30e)

**forward propagateç»“æœå¾—åˆ°costä¹‹åï¼Œå°±éœ€è¦è¿›è¡Œbackward propagateå»è·å¾—W1,b1,W2,b2å’Œgradientå‡†å¤‡å»è¿›è¡Œ gradient descent**


<a name='4-5'></a>
### 4.5 - Implement Backpropagation

Using the cache computed during forward propagation, you can now implement backward propagation.

<a name='ex-6'></a>
### Exercise 6 -  backward_propagation

Implement the function `backward_propagation()`.

**Instructions**:
Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You'll want to use the six equations on the right of this slide, since you are building a vectorized implementation.  
![21](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/b67ab817-cb16-455c-b245-d1653ef49575)


<caption><center><font color='purple'><b>Figure 1</b>: Backpropagation. Use the six equations on the right.</font></center></caption>

<!--
$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})$

$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $

$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}$

$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $

$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $

$\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}$

- Note that $*$ denotes elementwise multiplication.
- The notation you will use is common in deep learning coding:
    - dW1 = $\frac{\partial \mathcal{J} }{ \partial W_1 }$
    - db1 = $\frac{\partial \mathcal{J} }{ \partial b_1 }$
    - dW2 = $\frac{\partial \mathcal{J} }{ \partial W_2 }$
    - db2 = $\frac{\partial \mathcal{J} }{ \partial b_2 }$
    
!-->

- Tips:
    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute 
    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`.


ä¸ºäº†è®¡ç®—dZ1ï¼Œä½ éœ€è¦è®¡ç®—ğ‘”[1]â€²ï¼ˆğ‘[1]ï¼‰ã€‚ç”±äºğ‘”[1](.)æ˜¯tanhæ¿€æ´»å‡½æ•°ï¼Œå¦‚æœğ‘=ğ‘”[1](ğ‘§)ï¼Œé‚£ä¹ˆğ‘”[1]â€²(ğ‘§)=1-ğ‘2 ã€‚æ‰€ä»¥ä½ å¯ä»¥ç”¨ï¼ˆ1-np.power(A1, 2)ï¼‰æ¥è®¡ç®—ğ‘”[1]â€²ï¼ˆğ‘[1]ï¼‰ã€‚

æ¯”è¾ƒé‡è¦çš„å°±æ˜¯ï¼Œåœ¨tanhä½œä¸ºhidden layerçš„æ¿€æ´»å‡½æ•°ï¼Œè€Œsigmoidåšä¸ºoutput layerçš„æ¿€æ´»å‡½æ•°ï¼Œ
åœ¨è¿›è¡Œbackpropagationçš„æ—¶å€™ï¼Œå…ˆè®¡ç®—å‡ºoutput éƒ¨åˆ†çš„dW2, db2,dZ2.
ç„¶åå†ç”¨dZ2çš„ç»“æœç®—å‡ºdZ1,ç”±äºhiddenæ˜¯ç”¨tanh(),æ‰€ä»¥è®¡ç®—æ—¶å€™ï¼Œ dZ1 = W[2]TdZ2 * g[1]'(Z1)
æ ¹æ®æ¨å¯¼ï¼šg[1]'(Z1) = 1 - a^2.
è€Œåœ¨Numpyä¸­çš„np.power(æ•°ç»„ï¼Œæ¬¡æ–¹æ•°)å¯ä»¥å¸®åŠ©æˆ‘ä»¬å¯¹ä¸€ä¸ªæ•°ç»„å†…çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½è¿›è¡Œæ¬¡æ–¹è¿ç®—.

**Code**

       # GRADED FUNCTION: backward_propagation

       def backward_propagation(parameters, cache, X, Y):
           """
           Implement the backward propagation using the instructions above.
    
           Arguments:
           parameters -- python dictionary containing our parameters 
           cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
           X -- input data of shape (2, number of examples)
           Y -- "true" labels vector of shape (1, number of examples)
    
           Returns:
           grads -- python dictionary containing your gradients with respect to different parameters
           """
           m = X.shape[1]
    
           # First, retrieve W1 and W2 from the dictionary "parameters".
           #(â‰ˆ 2 lines of code)
           # W1 = ...
           # W2 = ...
           # YOUR CODE STARTS HERE
           W1 = parameters["W1"]
           W2 = parameters["W2"]
    
           # YOUR CODE ENDS HERE
        
           # Retrieve also A1 and A2 from dictionary "cache".
           #(â‰ˆ 2 lines of code)
           # A1 = ...
           # A2 = ...
           # YOUR CODE STARTS HERE
           A1 = cache["A1"]
           A2 = cache["A2"]
    
           # YOUR CODE ENDS HERE
    
           # Backward propagation: calculate dW1, db1, dW2, db2. 
           #(â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)
           # dZ2 = ...
           # dW2 = ...
           # db2 = ...
           # dZ1 = ...
           # dW1 = ...
           # db1 = ...
           # YOUR CODE STARTS HERE
           dZ2 = A2 - Y
           dW2 = 1/m * np.dot(dZ2, A1.T)
           db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)
           dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
           dW1 = 1/m * np.dot(dZ1, X.T)
           db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)
    
           # YOUR CODE ENDS HERE
    
           grads = {"dW1": dW1,
                    "db1": db1,
                    "dW2": dW2,
                    "db2": db2}
           
           return grads

![22](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/66d78cc7-716e-4fe7-9fd9-78bcd57a5354)

**åœ¨å¾—åˆ°äº†backward propagationä¹‹åå°±å¯ä»¥è¿›è¡Œå‚æ•°æ›´æ–°äº†ï¼Œgradient descent**

<a name='4-6'></a>
### 4.6 - Update Parameters 

<a name='ex-7'></a>
### Exercise 7 - update_parameters

Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).

**General gradient descent rule**: $\theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$ where $\alpha$ is the learning rate and $\theta$ represents a parameter.
ä¸€èˆ¬æ¢¯åº¦ä¸‹é™è§„åˆ™ï¼šğœƒ=ğœƒ-ğ›¼âˆ‚ğ½ï¼Œå…¶ä¸­ğ›¼æ˜¯å­¦ä¹ ç‡ï¼Œğœƒä»£è¡¨ä¸€ä¸ªå‚æ•°ã€‚

![sgd](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/8e182d5c-4264-4883-a50f-4f28c0da1ddb)
![sgd_bad](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/7b63007a-c389-4500-b9d9-c40e872767ba)


<caption><center><font color='purple'><b>Figure 2</b>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</font></center></caption>

**Hint**

- Use `copy.deepcopy(...)` when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.

å›¾2ï¼šæ¢¯åº¦ä¸‹é™ç®—æ³•çš„å­¦ä¹ ç‡å¥½ï¼ˆæ”¶æ•›ï¼‰å’Œå­¦ä¹ ç‡ä¸å¥½ï¼ˆå‘æ•£ï¼‰ã€‚å›¾ç‰‡ç”±Adam Harleyæä¾›ã€‚
å¥½çš„é‚£ä¸ªå›¾ï¼Œå­¦ä¹ ç‡æ˜æ˜¾ä½0.005ï¼Œè€Œå­¦ä¹ ç‡ä¸å¥½çš„è¿™ä¸ªé«˜0.05.
æ¸©é¦¨æç¤º

åœ¨å¤åˆ¶ä½œä¸ºå‚æ•°ä¼ é€’ç»™å‡½æ•°çš„åˆ—è¡¨æˆ–å­—å…¸æ—¶ï¼Œä½¿ç”¨copy.deepcopy(...)ã€‚å®ƒå¯ä»¥é¿å…è¾“å…¥å‚æ•°åœ¨å‡½æ•°ä¸­è¢«ä¿®æ”¹ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½æ˜¯ä½æ•ˆçš„ï¼Œä½†ä¸ºäº†è¯„åˆ†çš„ç›®çš„ï¼Œè¿™æ˜¯å¿…é¡»çš„ã€‚


**Coding**

        # GRADED FUNCTION: update_parameters

        def update_parameters(parameters, grads, learning_rate = 1.2):
            """
            Updates parameters using the gradient descent update rule given above
    
            Arguments:
            parameters -- python dictionary containing your parameters 
            grads -- python dictionary containing your gradients 
    
            Returns:
            parameters -- python dictionary containing your updated parameters 
            """
            # Retrieve a copy of each parameter from the dictionary "parameters". Use copy.deepcopy(...) for W1 and W2
            #(â‰ˆ 4 lines of code)
            # W1 = ...
            # b1 = ...
            # W2 = ...
            # b2 = ...
            # YOUR CODE STARTS HERE
            W1 = copy.deepcopy(parameters["W1"])
            b1 = copy.deepcopy(parameters["b1"])
            W2 = copy.deepcopy(parameters["W2"])
            b2 = copy.deepcopy(parameters["b2"])

    
            # YOUR CODE ENDS HERE
    
            # Retrieve each gradient from the dictionary "grads"
            #(â‰ˆ 4 lines of code)
            # dW1 = ...
            # db1 = ...
            # dW2 = ...
            # db2 = ...
            # YOUR CODE STARTS HERE
            dW1 = copy.deepcopy(grads["dW1"])
            db1 = copy.deepcopy(grads["db1"])
            dW2 = copy.deepcopy(grads["dW2"])
            db2 = copy.deepcopy(grads["db2"])

            # YOUR CODE ENDS HERE
    
            # Update rule for each parameter
            #(â‰ˆ 4 lines of code)
            # W1 = ...
            # b1 = ...
            # W2 = ...
            # b2 = ...
            # YOUR CODE STARTS HERE
            W1 = W1 - learning_rate * dW1
            b1 = b1 - learning_rate * db1
            W2 = W2 - learning_rate * dW2
            b2 = b2 - learning_rate * db2
    
            # YOUR CODE ENDS HERE
    
            parameters = {"W1": W1,
                          "b1": b1,
                          "W2": W2,
                          "b2": b2}
    
            return parameters
![23](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/f5ea6f73-4e44-48b4-8627-0356c1d5507d)

**ç°åœ¨å·²ç»å¾—åˆ°äº†æ•°æ®å’Œneural networkçš„shapeï¼Œä»¥åŠå®šä¹‰äº†éšæœºåŒ– initialize çš„parametersï¼šW1,W2ï¼Œä»¥åŠbiasï¼šb1ï¼Œb2, åŒæ—¶å®šä¹‰äº†forwardå’Œbackward propagationçš„å‡½æ•°ï¼Œæœ€åè¿˜å®šä¹‰å‡ºäº†æ›´æ–°W,çš„å‡½æ•°**
ä¸‹ä¸€æ­¥å°±æ˜¯é›†æˆï¼ˆintegrationï¼‰æ‰€æœ‰å‡½æ•°åˆ°nn.model()ä¸­.


æˆ‘ä»¬é¦–å…ˆæ ¹æ®è¾“å…¥æ•°æ®çš„ç»´åº¦å’Œéšè—å±‚å¤§å°ï¼Œ
ä½¿ç”¨ initialize_parameters å‡½æ•°åˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚ç„¶åï¼Œåœ¨å¾ªç¯ä¸­è¿›è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

æ­£å‘ä¼ æ’­ï¼šä½¿ç”¨ forward_propagation å‡½æ•°è®¡ç®—è¾“å‡º A2 å’Œç¼“å­˜ cacheã€‚
æˆæœ¬å‡½æ•°ï¼šä½¿ç”¨ compute_cost å‡½æ•°è®¡ç®—æˆæœ¬ã€‚
åå‘ä¼ æ’­ï¼šä½¿ç”¨ backward_propagation å‡½æ•°è®¡ç®—æ¢¯åº¦ gradsã€‚
å‚æ•°æ›´æ–°ï¼šä½¿ç”¨ update_parameters å‡½æ•°æ›´æ–°å‚æ•° parametersã€‚
å¦‚æœéœ€è¦ï¼Œæ¯éš”1000æ¬¡è¿­ä»£æ‰“å°æˆæœ¬ã€‚
æœ€åï¼Œè¿”å›å­¦ä¹ åˆ°çš„å‚æ•° parametersï¼Œè¿™äº›å‚æ•°å¯ä»¥ç”¨äºé¢„æµ‹ã€‚


åœ¨ä»£ç ä¸­ï¼Œnp.random.seed(3) çš„ä½œç”¨æ˜¯è®¾ç½®éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œä»¥ç¡®ä¿åœ¨æ¯æ¬¡è¿è¡Œä»£ç æ—¶éƒ½èƒ½å¾—åˆ°ç›¸åŒçš„éšæœºæ•°åºåˆ—ã€‚
ç§å­å€¼ä¸º3åªæ˜¯ä¸€ä¸ªéšæœºé€‰æ‹©çš„å¸¸æ•°ï¼Œä½ å¯ä»¥é€‰æ‹©ä»»ä½•å…¶ä»–æ•´æ•°ä½œä¸ºç§å­å€¼ï¼Œåªè¦ä½ åœ¨ä¸åŒåœ°æ–¹ä½¿ç”¨ç›¸åŒçš„ç§å­å€¼ï¼Œå°±èƒ½å¾—åˆ°ç›¸åŒçš„éšæœºæ•°åºåˆ—ã€‚
è¿™åœ¨è°ƒè¯•å’Œå¤ç°å®éªŒç»“æœæ—¶éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒç¡®ä¿ä»£ç çš„éšæœºéƒ¨åˆ†æ˜¯ç¡®å®šæ€§çš„ã€‚

è‡³äºæœ€åçš„ if å‡½æ•°ï¼Œå®ƒç”¨äºåœ¨æ¯æ¬¡è¿­ä»£çš„æ—¶å€™æ‰“å°æˆæœ¬ï¼ˆä»£ä»·ï¼‰ã€‚print_cost æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼å‚æ•°ï¼Œå¦‚æœè®¾ç½®ä¸º Trueï¼Œåˆ™åœ¨æ¯1000æ¬¡è¿­ä»£æ—¶ä¼šè¾“å‡ºå½“å‰è¿­ä»£æ¬¡æ•°å’Œå¯¹åº”çš„æˆæœ¬å€¼ã€‚
è¿™æ ·åšæ˜¯ä¸ºäº†æ–¹ä¾¿ç”¨æˆ·å®æ—¶ç›‘æµ‹æ¨¡å‹çš„è®­ç»ƒè¿›åº¦å’Œæˆæœ¬çš„å˜åŒ–æƒ…å†µï¼Œä»¥ä¾¿åœ¨éœ€è¦çš„æ—¶å€™è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚å¦‚æœä¸éœ€è¦åœ¨æ¯æ¬¡è¿­ä»£æ—¶æ‰“å°æˆæœ¬ï¼Œå¯ä»¥å°† print_cost å‚æ•°è®¾ç½®ä¸º Falseï¼Œåˆ™ä¸ä¼šè¾“å‡ºæˆæœ¬ä¿¡æ¯ã€‚

![24](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/c3fc61d6-568a-4847-8394-86430ed3e426)

**è¿›è¡Œæµ‹è¯•ï¼Œä½¿ç”¨predicåŠŸèƒ½**

<a name='5'></a>
## 5 - Test the Model

<a name='5-1'></a>
### 5.1 - Predict

<a name='ex-9'></a>
### Exercise 9 - predict

Predict with your model by building `predict()`.
Use forward propagation to predict results.

**Reminder**: predictions = $y_{prediction} = \mathbb 1 \text{{activation > 0.5}} = \begin{cases}
      1 & \text{if}\ activation > 0.5 \\
      0 & \text{otherwise}
    \end{cases}$  
    
As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```

é€šè¿‡å»ºç«‹predict()ï¼Œç”¨ä½ çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚ä½¿ç”¨å‰å‘ä¼ æ’­æ³•æ¥é¢„æµ‹ç»“æœï¼Œå¾—åˆ°A2å°±å¯ä»¥äº†ï¼Œå› ä¸ºè¾“å‡ºå±‚A2æ˜¯sigmoidå‡½æ•°ï¼Œæ˜¯é€»è¾‘åˆ¤æ–­çš„ã€‚

ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœä½ æƒ³æ ¹æ®ä¸€ä¸ªé˜ˆå€¼å°†ä¸€ä¸ªçŸ©é˜µXçš„æ¡ç›®è®¾ç½®ä¸º0å’Œ1ï¼Œä½ ä¼šè¿™æ ·åšï¼š X_new = (X > threshold)
æ‰€ä»¥åœ¨è¿™é‡Œé¢å¯ä»¥ç”¨ predictions = (A2 > 0.5)

åœ¨è¿™è¡Œä»£ç ä¸­ï¼ŒX_new = (X > threshold) æ˜¯ä¸€ä¸ªå¸ƒå°”è¡¨è¾¾å¼ï¼Œå®ƒå¯¹è¾“å…¥çŸ©é˜µ X è¿›è¡Œå…ƒç´ çº§æ¯”è¾ƒï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªç›¸åŒå½¢çŠ¶çš„å¸ƒå°”çŸ©é˜µ X_newã€‚

å…·ä½“æ¥è¯´ï¼Œ(X > threshold) è¡¨è¾¾å¼å°†å¯¹ X ä¸­çš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œæ¯”è¾ƒæ“ä½œï¼Œå¦‚æœå…ƒç´ çš„å€¼å¤§äº thresholdï¼Œåˆ™å¯¹åº”ä½ç½®çš„ç»“æœä¸º Trueï¼Œå¦åˆ™ä¸º Falseã€‚ç”Ÿæˆçš„å¸ƒå°”çŸ©é˜µ X_new ä¸ X å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼Œä½†å…¶å…ƒç´ çš„å€¼ä¸ºå¸ƒå°”ç±»å‹ã€‚

è¿™ç§æ“ä½œå¸¸ç”¨äºå°†è¿ç»­å€¼è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ‡å¿—æˆ–è¿›è¡Œé˜ˆå€¼å¤„ç†ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ (X > 0.5) å°†è¿ç»­å€¼çŸ©é˜µ X è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ‡å¿—çŸ©é˜µï¼Œå…¶ä¸­å¤§äº 0.5 çš„å…ƒç´ ä¸º Trueï¼Œå°äºç­‰äº 0.5 çš„å…ƒç´ ä¸º Falseã€‚

æ³¨æ„ï¼ŒX å’Œ threshold çš„å½¢çŠ¶éœ€è¦ç›¸åŒ¹é…ï¼Œå¦åˆ™å¯èƒ½ä¼šå¼•å‘é”™è¯¯ã€‚

      # GRADED FUNCTION: predict

      def predict(parameters, X):
          """
          Using the learned parameters, predicts a class for each example in X
    
          Arguments:
          parameters -- python dictionary containing your parameters 
          X -- input data of size (n_x, m)
    
          Returns
          predictions -- vector of predictions of our model (red: 0 / blue: 1)
          """
    
          # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
          #(â‰ˆ 2 lines of code)
          # A2, cache = ...
          # predictions = ...
          # YOUR CODE STARTS HERE
          A2, cache = forward_propagation(X, parameters)
          predictions = (A2 > 0.5)
    
          # YOUR CODE ENDS HERE
    
          return predictions

é€šè¿‡ä¾‹å­æ¥åˆ¤æ–­æ˜¯å¦å·¥ä½œ
![25](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/ac056690-2c8f-4fc7-8af9-c8ed10f524a9)

<a name='5-2'></a>
### 5.2 - Test the Model on the Planar Dataset

It's time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units!

5.2 - åœ¨å¹³é¢æ•°æ®é›†ä¸Šæµ‹è¯•æ¨¡å‹
ç°åœ¨æ˜¯æ—¶å€™è¿è¡Œæ¨¡å‹ï¼Œçœ‹çœ‹å®ƒåœ¨å¹³é¢æ•°æ®é›†ä¸Šçš„è¡¨ç°äº†ã€‚è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œç”¨ğ‘›â„éšè—å•å…ƒçš„å•ä¸€éšè—å±‚æµ‹è¯•ä½ çš„æ¨¡å‹!

        # Build a model with a n_h-dimensional hidden layer
        parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)

        # Plot the decision boundary
        plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
        plt.title("Decision Boundary for hidden layer size " + str(4))
![26](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/e351c335-8031-4e61-b43a-72512b107913)


    è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¹‹å‰å®šä¹‰çš„ nn_model å‡½æ•°æ¥å»ºç«‹ä¸€ä¸ªå…·æœ‰ n_h ç»´åº¦éšè—å±‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚å‚æ•° X å’Œ Y æ˜¯è¾“å…¥æ•°æ®å’Œæ ‡ç­¾ï¼Œn_h æ˜¯éšè—å±‚çš„ç»´åº¦ï¼Œnum_iterations æ˜¯æ¢¯åº¦ä¸‹é™ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬è°ƒç”¨ nn_model å‡½æ•°ï¼Œå®ƒä¼šè¿›è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

åˆå§‹åŒ–å‚æ•°ï¼šæ ¹æ®è¾“å…¥æ•°æ®çš„ç»´åº¦å’Œéšè—å±‚ç»´åº¦ï¼Œä½¿ç”¨éšæœºå€¼åˆå§‹åŒ–æƒé‡å’Œåç½®ã€‚
åœ¨æ¢¯åº¦ä¸‹é™å¾ªç¯ä¸­ï¼Œè¿›è¡Œå‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°çš„æ­¥éª¤ã€‚
åœ¨å¾ªç¯çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬ä¼šè®¡ç®—æŸå¤±å¹¶æ‰“å°å‡ºæ¥ï¼ˆç”± print_cost=True æ§åˆ¶ï¼‰ï¼Œä»¥ä¾¿è§‚å¯ŸæŸå¤±å‡½æ•°çš„å˜åŒ–æƒ…å†µã€‚

æ¥ç€ï¼Œæˆ‘ä»¬è°ƒç”¨ plot_decision_boundary å‡½æ•°æ¥ç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚è¿™ä¸ªå‡½æ•°çš„å‚æ•°æ˜¯ä¸€ä¸ªå‡½æ•°å’Œæ•°æ®é›† X å’Œ Yã€‚å®ƒä¼šæ ¹æ®è¿™ä¸ªå‡½æ•°é¢„æµ‹çš„ç»“æœï¼Œç»˜åˆ¶å‡ºæ•°æ®ç‚¹å’Œå†³ç­–è¾¹ç•Œã€‚æˆ‘ä»¬ä½¿ç”¨ lambda x: predict(parameters, x.T) ä½œä¸ºå‡½æ•°ï¼Œå…¶ä¸­ predict(parameters, x.T) ç”¨äºé¢„æµ‹è¾“å…¥æ•°æ® x.T çš„æ ‡ç­¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å°† X å’Œ Y æ•°æ®é›†ä¼ é€’ç»™ plot_decision_boundary å‡½æ•°ï¼Œå®ƒä¼šæ ¹æ®æ¨¡å‹çš„é¢„æµ‹ç»“æœç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚

æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ plt.title ç»™ç»˜åˆ¶çš„å›¾åƒæ·»åŠ æ ‡é¢˜ï¼ŒæŒ‡æ˜äº†éšè—å±‚çš„ç»´åº¦ n_h æ˜¯å¤šå°‘ã€‚

**è®¡ç®—å‡ºå‡†ç¡®åº¦**

        # Print accuracy
        predictions = predict(parameters, X)
        print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')
        
![27](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/7820a56e-c9b0-497e-8a33-65aa0d3ca2dc)

è¿™æ®µä»£ç ç”¨äºè®¡ç®—å¹¶æ‰“å°æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬è°ƒç”¨ predict å‡½æ•°ï¼Œä¼ å…¥å‚æ•° parameters å’Œè¾“å…¥æ•°æ® Xï¼Œå¾—åˆ°å¯¹è¾“å…¥æ•°æ®çš„é¢„æµ‹ç»“æœ predictionsã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨å‘é‡åŒ–çš„æ–¹æ³•è®¡ç®—å‡†ç¡®ç‡ã€‚é€šè¿‡ np.dot(Y, predictions.T)ï¼Œæˆ‘ä»¬è®¡ç®—äº†é¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾çš„ç‚¹ç§¯ï¼Œ
å¾—åˆ°é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°é‡ã€‚é€šè¿‡ np.dot(1 - Y, 1 - predictions.T)ï¼Œæˆ‘ä»¬è®¡ç®—äº†é¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾å–åçš„ç‚¹ç§¯ï¼Œ
å¾—åˆ°é¢„æµ‹é”™è¯¯çš„æ ·æœ¬æ•°é‡ã€‚å°†è¿™ä¸¤ä¸ªæ•°é‡ç›¸åŠ ï¼Œé™¤ä»¥æ€»æ ·æœ¬æ•°é‡ Y.sizeï¼Œå†ä¹˜ä»¥ 100ï¼Œå³å¯å¾—åˆ°å‡†ç¡®ç‡çš„ç™¾åˆ†æ¯”ã€‚

æœ€åï¼Œä½¿ç”¨ print å‡½æ•°æ‰“å°å‡†ç¡®ç‡çš„ç»“æœã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™æ®µä»£ç çš„ç›®çš„æ˜¯è®¡ç®—å¹¶æ‰“å°æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å‡†ç¡®ç‡ã€‚

        

ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™æ®µä»£ç çš„ç›®çš„æ˜¯å»ºç«‹ä¸€ä¸ªå…·æœ‰ n_h ç»´åº¦éšè—å±‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¹¶å¯è§†åŒ–æ¨¡å‹çš„å†³ç­–è¾¹ç•Œï¼Œä»è€ŒæŸ¥çœ‹æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„åˆ†ç±»æ•ˆæœã€‚


ä¸é€»è¾‘å›å½’ç›¸æ¯”ï¼Œå‡†ç¡®ç‡ç¡®å®å¾ˆé«˜ã€‚è¯¥æ¨¡å‹å·²ç»å­¦ä¼šäº†èŠ±ç“£çš„æ¨¡å¼! ä¸é€»è¾‘å›å½’ä¸åŒï¼Œç¥ç»ç½‘ç»œç”šè‡³èƒ½å¤Ÿå­¦ä¹ é«˜åº¦éçº¿æ€§çš„å†³ç­–è¾¹ç•Œã€‚

ä¸‹é¢æ˜¯å¯¹ä½ åˆšåˆšå®Œæˆçš„æ‰€æœ‰å·¥ä½œçš„ä¸€ä¸ªç®€å•å›é¡¾ï¼š

å»ºç«‹äº†ä¸€ä¸ªå®Œæ•´çš„å¸¦æœ‰éšè—å±‚çš„2ç±»åˆ†ç±»ç¥ç»ç½‘ç»œ
å¾ˆå¥½åœ°åˆ©ç”¨äº†ä¸€ä¸ªéçº¿æ€§å•å…ƒ
è®¡ç®—äº†äº¤å‰ç†µæŸå¤±
å®ç°äº†å‰å‘å’Œåå‘ä¼ æ’­
çœ‹åˆ°äº†æ”¹å˜éšè—å±‚å¤§å°çš„å½±å“ï¼ŒåŒ…æ‹¬è¿‡æ‹Ÿåˆã€‚
ä½ å·²ç»åˆ›å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿå­¦ä¹ æ¨¡å¼çš„ç¥ç»ç½‘ç»œ! ä¼˜ç§€çš„å·¥ä½œã€‚ä¸‹é¢æ˜¯ä¸€äº›å¯é€‰çš„ç»ƒä¹ ï¼Œä»¥å°è¯•å…¶ä»–éšè—å±‚å¤§å°å’Œå…¶ä»–æ•°æ®é›†ã€‚

**å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç æ¥æµ‹è¯•ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡ ä¸ªç¥ç»å…ƒè¢«åŒ…å«åœ¨hidden layerä¸­çš„æ—¶å€™ï¼Œç»“æœæœ€å‡†ç¡®**

       # This may take about 2 minutes to run

       plt.figure(figsize=(16, 32))
       hidden_layer_sizes = [1, 2, 3, 4, 5, 20]
       # hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
       for i, n_h in enumerate(hidden_layer_sizes):
           plt.subplot(5, 2, i+1)
           plt.title('Hidden Layer of size %d' % n_h)
           parameters = nn_model(X, Y, n_h, num_iterations = 5000)
           plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
           predictions = predict(parameters, X)
           accuracy = float((np.dot(Y,predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size)*100)
           print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))

  ![28](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/543f3a93-83f5-4793-ac70-a3b82c7a1ec7)
  
![29](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/e6fa4501-6f89-4ec2-be73-16e687bc9dbb)


### è§£é‡Šï¼š

- è¾ƒå¤§çš„æ¨¡å‹ï¼ˆæœ‰æ›´å¤šçš„éšè—å•å…ƒï¼‰èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”è®­ç»ƒé›†ï¼Œç›´åˆ°æœ€ç»ˆæœ€å¤§çš„æ¨¡å‹è¿‡åº¦é€‚åº”æ•°æ®ã€‚
- æœ€å¥½çš„éšè—å±‚å¤§å°ä¼¼ä¹æ˜¯n_h=5å·¦å³ã€‚äº‹å®ä¸Šï¼Œåœ¨æ­¤é™„è¿‘çš„æ•°å€¼ä¼¼ä¹å¯ä»¥å¾ˆå¥½åœ°æ‹Ÿåˆæ•°æ®ï¼Œè€Œä¸ä¼šäº§ç”Ÿæ˜æ˜¾çš„è¿‡æ‹Ÿåˆã€‚
- ç¨åï¼Œä½ å°†ç†Ÿæ‚‰æ­£åˆ™åŒ–ï¼Œå®ƒå¯ä»¥è®©ä½ ä½¿ç”¨éå¸¸å¤§çš„æ¨¡å‹ï¼ˆå¦‚n_h=50ï¼‰è€Œä¸è‡³äºè¿‡åº¦æ‹Ÿåˆã€‚



å½“ä½ æŠŠtanhæ¿€æ´»æ”¹ä¸ºsigmoidæ¿€æ´»æˆ–ReLUæ¿€æ´»æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
ç©å¼„ä¸€ä¸‹å­¦ä¹ ç‡ã€‚ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
å¦‚æœæˆ‘ä»¬æ”¹å˜æ•°æ®é›†å‘¢ï¼Ÿ(è§ä¸‹é¢ç¬¬7éƒ¨åˆ†ï¼)
