 ### å®ç°ä¸€ä¸ªå…·æœ‰å•ä¸€éšè—å±‚çš„2ç±»åˆ†ç±»ç¥ç»ç½‘ç»œ###

- ä½¿ç”¨å…·æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°çš„å•å…ƒï¼Œå¦‚tanh
- è®¡ç®—äº¤å‰ç†µæŸå¤±
- å®ç°å‰å‘å’Œåå‘ä¼ æ’­

- <a name='1'></a>
# 1 - Packages

First import all the packages that you will need during this assignment.

- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.
- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. 
- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python.
- testCases provides some test examples to assess the correctness of your functions
- planar_utils provide various useful functions used in this assignment

  é¦–å…ˆå¯¼å…¥æ‰€æœ‰ä½ åœ¨è¿™æ¬¡ä½œä¸šä¸­éœ€è¦çš„åŒ…ã€‚

- numpyæ˜¯ç”¨Pythonè¿›è¡Œç§‘å­¦è®¡ç®—çš„åŸºæœ¬åŒ…ã€‚
- sklearnä¸ºæ•°æ®æŒ–æ˜å’Œæ•°æ®åˆ†ææä¾›äº†ç®€å•è€Œæœ‰æ•ˆçš„å·¥å…·ã€‚
- matplotlibæ˜¯ä¸€ä¸ªç”¨äºåœ¨Pythonä¸­ç»˜åˆ¶å›¾å½¢çš„åº“ã€‚
- testCases æä¾›äº†ä¸€äº›æµ‹è¯•å®ä¾‹ï¼Œä»¥è¯„ä¼°ä½ çš„å‡½æ•°çš„æ­£ç¡®æ€§ã€‚
- planar_utilsæä¾›äº†æœ¬ä½œä¸šä¸­ä½¿ç”¨çš„å„ç§æœ‰ç”¨çš„å‡½æ•°


  **Coding**

        # Package imports
        import numpy as np
        import copy
        import matplotlib.pyplot as plt
        from testCases_v2 import *
        from public_tests import *
        import sklearn
        import sklearn.datasets
        import sklearn.linear_model
        from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

        %matplotlib inline

        %load_ext autoreload
        %autoreload 2

  <a name='2'></a>
# 2 - Load the Dataset 

        X, Y = load_planar_dataset()

ä½¿ç”¨matplotlibå¯¹æ•°æ®é›†è¿›è¡Œå¯è§†åŒ–ã€‚è¯¥æ•°æ®çœ‹èµ·æ¥åƒä¸€æœµ "èŠ±"ï¼Œæœ‰ä¸€äº›çº¢è‰²ï¼ˆæ ‡ç­¾y=0ï¼‰å’Œä¸€äº›è“è‰²ï¼ˆy=1ï¼‰çš„ç‚¹ã€‚ä½ çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªæ¨¡å‹æ¥é€‚åº”è¿™ä¸ªæ•°æ®ã€‚
æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ†ç±»å™¨èƒ½å°†åŒºåŸŸå®šä¹‰ä¸ºçº¢è‰²æˆ–è“è‰²ã€‚

![11](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/d987eef8-ce10-478c-b68d-e768e25673da)

![12](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/5c910039-0b9e-4964-aa43-995ec9941fcf)

You have:

- a numpy-array (matrix) X that contains your features (x1, x2)
- a numpy-array (vector) Y that contains your labels (red:0, blue:1).
First, get a better sense of what your data is like.

### åˆ¤æ–­Xï¼ˆç‰¹å¾ï¼‰å’ŒY(æ ‡ç­¾)çš„æ•°é‡æœ‰å¤šå°‘

         # (â‰ˆ 3 lines of code)
         # shape_X = ...
         # shape_Y = ...
         # training set size
         # m = ...
         # YOUR CODE STARTS HERE
         shape_X = X.shape
         shape_Y = Y.shape
         m = X.shape[1]
         # YOUR CODE ENDS HERE

         print ('The shape of X is: ' + str(shape_X))
         print ('The shape of Y is: ' + str(shape_Y))
         print ('I have m = %d training examples!' % (m))

![13](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/5e8b8865-ba73-45a1-b3de-b83cc5ea7932)


### 3 - ç®€å•é€»è¾‘å›å½’
åœ¨æ„å»ºå®Œæ•´çš„ç¥ç»ç½‘ç»œä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆçœ‹çœ‹é€»è¾‘å›å½’å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ sklearn çš„å†…ç½®å‡½æ•°æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚è¿è¡Œä¸‹é¢çš„ä»£ç ä»¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨ã€‚

![14](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/d3c48807-7595-4bc1-b7de-d4243a2d2bbc)

![15](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/5a43732d-3c9a-4949-bd7a-bb7b63f28f75)

<a name='4'></a>
## 4 - Neural Network model

Logistic regression didn't work well on the flower dataset. Next, you're going to train a Neural Network with a single hidden layer and see how that handles the same problem.

**The model**:
![16](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/1f6225ac-b4c1-4984-b696-675bf59793a6)



**Reminder**: The general methodology to build a Neural Network is to:
    1. Define the neural network structure ( # of input units,  # of hidden units, etc). 
    2. Initialize the model's parameters
    3. Loop:
        - Implement forward propagation
        - Compute loss
        - Implement backward propagation to get the gradients
        - Update parameters (gradient descent)


**æé†’**ï¼š æ„å»ºç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ–¹æ³•æ˜¯ï¼šï¼š
    1. å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„ï¼ˆè¾“å…¥å•å…ƒçš„æ•°é‡ï¼Œéšè—å•å…ƒçš„æ•°é‡ç­‰ï¼‰ã€‚
    2. åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
    3. å¾ªç¯ï¼š
        - å®æ–½å‰å‘ä¼ æ’­
        - è®¡ç®—æŸå¤±
        - å®æ–½åå‘ä¼ æ’­ä»¥è·å¾—æ¢¯åº¦
        - æ›´æ–°å‚æ•°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰

In practice, you'll often build helper functions to compute steps 1-3, then merge them into one function called `nn_model()`. Once you've built `nn_model()` and learned the right parameters, you can make predictions on new data.


åœ¨å®è·µä¸­ï¼Œä½ é€šå¸¸ä¼šå»ºç«‹è¾…åŠ©å‡½æ•°æ¥è®¡ç®—ç¬¬1-3æ­¥ï¼Œç„¶åå°†å®ƒä»¬åˆå¹¶åˆ°ä¸€ä¸ªå«åš`nn_model()`çš„å‡½æ•°ä¸­ã€‚ä¸€æ—¦ä½ å»ºç«‹äº†`nn_model()`å¹¶å­¦ä¹ äº†æ­£ç¡®çš„å‚æ•°ï¼Œä½ å°±å¯ä»¥å¯¹æ–°çš„æ•°æ®è¿›è¡Œé¢„æµ‹äº†ã€‚


<a name='4-1'></a>
### 4.1 - Defining the neural network structure ####

<a name='ex-2'></a>
### Exercise 2 - layer_sizes 

Define three variables:
    - n_x: the size of the input layer
    - n_h: the size of the hidden layer (**set this to 4, only for this Exercise 2**) 
    - n_y: the size of the output layer

**Hint**: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.

4.1 - å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„

ç»ƒä¹ 2--å±‚_å°ºå¯¸
å®šä¹‰ä¸‰ä¸ªå˜é‡ï¼š

- n_x: è¾“å…¥å±‚çš„å¤§å°
- n_hï¼šéšè—å±‚çš„å¤§å°ï¼ˆ**å°†å…¶è®¾ä¸º4ï¼Œä»…ç”¨äºæœ¬ç»ƒä¹ 2**ï¼‰ã€‚
- n_yï¼šè¾“å‡ºå±‚çš„å¤§å°ã€‚
- 
æç¤ºï¼šä½¿ç”¨Xå’ŒYçš„å½¢çŠ¶æ¥å¯»æ‰¾n_xå’Œn_yã€‚åŒæ—¶ï¼Œå°†éšè—å±‚çš„å¤§å°ç¡¬ç¼–ç ä¸º4ã€‚

åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¾“å…¥å±‚çš„ç¥ç»å…ƒæ•°é‡ç”±ç‰¹å¾æ•°æ®é›†çš„ç»´åº¦å†³å®šã€‚æ¯ä¸ªç‰¹å¾åœ¨ç¥ç»ç½‘ç»œä¸­å¯¹åº”ä¸€ä¸ªè¾“å…¥ç¥ç»å…ƒã€‚ä¾‹å¦‚ï¼Œå¦‚æœç‰¹å¾æ•°æ®é›†Xçš„å½¢çŠ¶ä¸º(n_x, m)ï¼Œå…¶ä¸­n_xè¡¨ç¤ºç‰¹å¾çš„æ•°é‡ï¼Œ
mè¡¨ç¤ºæ ·æœ¬çš„æ•°é‡ï¼Œé‚£ä¹ˆè¾“å…¥å±‚çš„ç¥ç»å…ƒæ•°é‡å°±æ˜¯n_xã€‚

è¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°é‡é€šå¸¸ç”±ä»»åŠ¡çš„è¦æ±‚å†³å®šã€‚åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œè¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°é‡é€šå¸¸ä¸ç±»åˆ«çš„æ•°é‡ç›¸åŒï¼Œæ¯ä¸ªè¾“å‡ºç¥ç»å…ƒå¯¹åº”ä¸€ä¸ªç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œå¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºå±‚æœ‰ä¸¤ä¸ªç¥ç»å…ƒï¼Œ
åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªç±»åˆ«ã€‚å¯¹äºå¤šåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°é‡ç­‰äºç±»åˆ«çš„æ•°é‡ã€‚

å› æ­¤ï¼Œè¾“å…¥å±‚å’Œè¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°é‡æ˜¯æ ¹æ®é—®é¢˜çš„ç‰¹æ€§å’Œè¦æ±‚æ¥ç¡®å®šçš„ï¼Œè€Œéšè—å±‚çš„ç¥ç»å…ƒæ•°é‡å¯ä»¥æ ¹æ®ç½‘ç»œæ¶æ„å’Œå®éªŒéœ€æ±‚è¿›è¡Œè®¾ç½®ã€‚

**Code**

# GRADED FUNCTION: layer_sizes

        def layer_sizes(X, Y):
    """
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    """
    #(â‰ˆ 3 lines of code)
    # n_x = ... 
    # n_h = ...
    # n_y = ... 
    # YOUR CODE STARTS HERE
    n_x = X.shape[0]
    n_h = 4
    n_y = Y.shape[0]
    
    # YOUR CODE ENDS HERE
    return (n_x, n_h, n_y)

     # è·å–layer_sizesçš„æµ‹è¯•æ¡ˆä¾‹ï¼Œ è¿™ä¸ªæ¡ˆä¾‹ä¸­çš„æ•°æ®å’Œä¹‹å‰çš„é‚£ä¸ªæ— å…³
     t_X, t_Y = layer_sizes_test_case()

     # è®¡ç®—å±‚çš„å¤§å°
     (n_x, n_h, n_y) = layer_sizes(t_X, t_Y)

     # æ‰“å°è¾“å…¥å±‚çš„å¤§å°
     print("è¾“å…¥å±‚çš„å¤§å°ä¸ºï¼šn_x = " + str(n_x))

     # æ‰“å°éšè—å±‚çš„å¤§å°
     print("éšè—å±‚çš„å¤§å°ä¸ºï¼šn_h = " + str(n_h))

     # æ‰“å°è¾“å‡ºå±‚çš„å¤§å°
     print("è¾“å‡ºå±‚çš„å¤§å°ä¸ºï¼šn_y = " + str(n_y))

     # è¿è¡Œlayer_sizesçš„æµ‹è¯•
     layer_sizes_test(layer_sizes)

![17](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/6fb1cf5e-6790-44bb-8c29-6a7e1d9f8b46)


### åœ¨ç¡®å®šå®Œshallowç¥ç»ç½‘ç»œçš„structureä¹‹åï¼Œå°±æ˜¯å¯¹æ•°æ®è¿›è¡Œinitializingã€‚

ä¸€èˆ¬ä½¿ç”¨éšæœºè¿˜åŸæ³•ï¼Œnp.random.randn(a,b) * 0.01ï¼Œä¸é€‚ç”¨å½’0æ³•æ˜¯å› ä¸ºï¼Œå¦‚æœhidden layeréƒ½æ˜¯å½’0åˆ™ï¼Œéšè—å±‚æ²¡æ„ä¹‰äº†.
<a name='4-2'></a>
### 4.2 - Initialize the model's parameters ####

<a name='ex-3'></a>
### Exercise 3 -  initialize_parameters

Implement the function `initialize_parameters()`.

**Instructions**:
- Make sure your parameters' sizes are right. Refer to the neural network figure above if needed.
- You will initialize the weights matrices with random values. 
    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).
- You will initialize the bias vectors as zeros. 
    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros.
 
**ä»£ç è§£é‡Š**


W1 = np.random.randn(n_h, n_x) * 0.01 
**å…¶ä¸­ï¼Œn_hæ˜¯è¿™ä¸ªarrayçš„è¡Œï¼Œè¡¨ç¤ºçš„æ˜¯ç‰¹å¾ï¼Œæ‰€ä»¥è¡¨ç¤ºè¿™ä¸ªlayerç¥ç»å…ƒçš„æ•°é‡ï¼Œè€Œn_xè¡¨ç¤ºçš„æ˜¯åˆ—ï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸€ä¸ªå±‚çº§inputåˆ°è¿™ä¸ªå±‚çº§çš„æ•°é‡**
  
b1 = np.zeros((n_h,1))
**bæ˜¯ä¸€ä¸ªå¸¸æ•°åˆ—ï¼Œä»–æ˜¯ä¸å—ä¸Šä¸€å±‚çº§å½±åƒçš„ï¼Œæ‰€ä»¥åé¢æ˜¯1**
W2 = np.random.randn(n_y, n_h) * 0.01
 b2 = np.zeros((n_y,1))


        # GRADED FUNCTION: initialize_parameters

        def initialize_parameters(n_x, n_h, n_y):
            """
            Argument:
            n_x -- size of the input layer
            n_h -- size of the hidden layer
            n_y -- size of the output layer
    
            Returns:
            params -- python dictionary containing your parameters:
                            W1 -- weight matrix of shape (n_h, n_x)
                            b1 -- bias vector of shape (n_h, 1)
                            W2 -- weight matrix of shape (n_y, n_h)
                            b2 -- bias vector of shape (n_y, 1)
            """    
            #(â‰ˆ 4 lines of code)
            # W1 = ...
            # b1 = ...
            # W2 = ...
            # b2 = ...
            # YOUR CODE STARTS HERE
            W1 = np.random.randn(n_h, n_x) * 0.01
            b1 = np.zeros((n_h,1))
            W2 = np.random.randn(n_y, n_h) * 0.01
            b2 = np.zeros((n_y,1))
    
            # YOUR CODE ENDS HERE

            parameters = {"W1": W1,
                          "b1": b1,
                          "W2": W2,
                          "b2": b2}
    
            return parameters

     
![18](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/12dba306-5c22-457e-830b-fe96df0940b0)


åœ¨å®šä¹‰å®Œå±‚çº§ï¼Œ
å®šä¹‰å®Œinitailizeå‡½æ•°ä¹‹åï¼Œå°±å¯ä»¥å®šä¹‰forward propagate å‡½æ•°äº†.

<a name='4-3'></a>
### 4.3 - The Loop 

<a name='ex-4'></a>
### Exercise 4 - forward_propagation

Implement `forward_propagation()` using the following equations:

$$Z^{[1]} =  W^{[1]} X + b^{[1]}\tag{1}$$ 
$$A^{[1]} = \tanh(Z^{[1]})\tag{2}$$
$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\tag{3}$$
$$\hat{Y} = A^{[2]} = \sigma(Z^{[2]})\tag{4}$$


**Instructions**:

- Check the mathematical representation of your classifier in the figure above.
- Use the function `sigmoid()`. It's built into (imported) this notebook.
- Use the function `np.tanh()`. It's part of the numpy library.
- Implement using these steps:
    1. Retrieve each parameter from the dictionary "parameters" (which is the output of `initialize_parameters()` by using `parameters[".."]`.
    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).
- Values needed in the backpropagation are stored in "cache". The cache will be given as an input to the backpropagation function.

è¿™æ¬¡çš„hidden layerä¸­ä½¿ç”¨çš„æ˜¯ï¼Œnumpy libraryä¸­çš„np.tanh()å‡½æ•°ï¼Œå¹¶ä¸”tanh as the active function for hidden layer.


**Coding**

        # GRADED FUNCTION:forward_propagation

        def forward_propagation(X, parameters):
            """
            Argument:
            X -- input data of size (n_x, m)
            parameters -- python dictionary containing your parameters (output of initialization function)
    
            Returns:
            A2 -- The sigmoid output of the second activation
            cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
            """
            # Retrieve each parameter from the dictionary "parameters"
            #(â‰ˆ 4 lines of code)
            # W1 = ...
            # b1 = ...
            # W2 = ...
            # b2 = ...
            # YOUR CODE STARTS HERE
            W1 = parameters["W1"]
            b1 = parameters["b1"]
            W2 = parameters["W2"]
            b2 = parameters["b2"]
    
            # YOUR CODE ENDS HERE
    
            # Implement Forward Propagation to calculate A2 (probabilities)
            # (â‰ˆ 4 lines of code)
            # Z1 = ...
            # A1 = ...
            # Z2 = ...
            # A2 = ...
            # YOUR CODE STARTS HERE
            Z1 = np.dot(W1,X) + b1
            A1 = np.tanh(Z1)
            Z2 = np.dot(W2,A1) + b2
            A2 = 1/(1+np.exp(-Z2))
    
            # YOUR CODE ENDS HERE
    
            assert(A2.shape == (1, X.shape[1]))
    
            cache = {"Z1": Z1,
                     "A1": A1,
                     "Z2": Z2,
                     "A2": A2}
    
            return A2, cache

**ä»¥ä¸‹æ˜¯ä»æµ‹è¯•æ•°é›†ä¸­æå–çš„**

![19](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/11dbc883-2b95-4071-85f0-751a2dec57d9)

<a name='4-4'></a>
### 4.4 - Compute the Cost

Now that you've computed $A^{[2]}$ (in the Python variable "`A2`"), which contains $a^{[2](i)}$ for all examples, you can compute the cost function as follows:

$$J = - \frac{1}{m} \sum\limits_{i = 1}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}$$

<a name='ex-5'></a>
### Exercise 5 - compute_cost 

Implement `compute_cost()` to compute the value of the cost $J$.

**Instructions**:
- There are many ways to implement the cross-entropy loss. This is one way to implement one part of the equation without for loops:  âˆ’âˆ‘ğ‘–=1ğ‘šğ‘¦(ğ‘–)log(ğ‘[2](ğ‘–)) :
  
```python
logprobs = np.multiply(np.log(A2),Y)
cost = - np.sum(logprobs)          
```

- Use that to build the whole expression of the cost function.

**Notes**: 

- You can use either `np.multiply()` and then `np.sum()` or directly `np.dot()`).  
- If you use `np.multiply` followed by `np.sum` the end result will be a type `float`, whereas if you use `np.dot`, the result will be a 2D numpy array.  
- You can use `np.squeeze()` to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array). 
- You can also cast the array as a type `float` using `float()`.


- ä½ å¯ä»¥ä½¿ç”¨np.multiply()ç„¶ånp.sum()æˆ–è€…ç›´æ¥ä½¿ç”¨np.dot())ã€‚
- å¦‚æœä½ ä½¿ç”¨np.multiplyï¼Œç„¶åå†ä½¿ç”¨np.sumï¼Œæœ€ç»ˆçš„ç»“æœå°†æ˜¯ä¸€ä¸ªæµ®ç‚¹ç±»å‹ï¼Œè€Œå¦‚æœä½ ä½¿ç”¨np.dotï¼Œç»“æœå°†æ˜¯ä¸€ä¸ª2Dçš„numpyæ•°ç»„ã€‚
- ä½ å¯ä»¥ä½¿ç”¨np.squeeze()æ¥å»é™¤å¤šä½™çš„ç»´åº¦ï¼ˆå¦‚æœæ˜¯å•ä¸€çš„floatï¼Œè¿™å°†è¢«å‡å°‘ä¸ºä¸€ä¸ªé›¶ç»´æ•°ç»„ï¼‰ã€‚
- ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨float()å°†æ•°ç»„è½¬æ¢ä¸ºfloatç±»å‹ã€‚

  ä»£ç è§£é‡Šï¼š

logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y): è¿™è¡Œä»£ç è®¡ç®—äº†A2çš„å¯¹æ•°ä¸Yçš„å…ƒç´ çº§ä¹˜ç§¯ï¼Œä»¥åŠ(1 - A2)çš„å¯¹æ•°ä¸(1 - Y)çš„å…ƒç´ çº§ä¹˜ç§¯ã€‚å®ƒè®¡ç®—äº†é¢„æµ‹å€¼ï¼ˆA2ï¼‰å’ŒçœŸå®å€¼ï¼ˆYï¼‰çš„å¯¹æ•°æ¦‚ç‡ã€‚

cost = - np.sum(logprobs) / m: è¿™è¡Œä»£ç é€šè¿‡å¯¹æ‰€æœ‰å¯¹æ•°æ¦‚ç‡è¿›è¡Œæ±‚å’Œå¹¶é™¤ä»¥ç¤ºä¾‹æ•°é‡mï¼Œè®¡ç®—äº†å¹³å‡äº¤å‰ç†µæˆæœ¬ã€‚è´Ÿå·ç”¨äºç¿»è½¬æ±‚å’Œçš„ç¬¦å·ï¼Œå› ä¸ºäº¤å‰ç†µæˆæœ¬åœ¨æ–¹ç¨‹ä¸­å®šä¹‰ä¸ºè´Ÿæ•°ã€‚

cost = float(np.squeeze(cost)): è¿™è¡Œä»£ç é€šè¿‡å‹ç¼©æ“ä½œå°†å½¢çŠ¶ä¸ºï¼ˆ1ï¼Œ1ï¼‰çš„äºŒç»´æ•°ç»„çš„æˆæœ¬è½¬æ¢ä¸ºæ ‡é‡å€¼ã€‚np.squeeze()å‡½æ•°ä¼šåˆ é™¤å¤§å°ä¸º1çš„ä»»ä½•ç»´åº¦ï¼Œæ‰€ä»¥å®ƒå°†[[17]]è½¬æ¢ä¸º17ï¼ˆæ ‡é‡å€¼ï¼‰ã€‚

æœ€åï¼Œcostå˜é‡ä½œä¸ºå‡½æ•°çš„è¾“å‡ºè¿”å›ã€‚

æ€»ä½“è€Œè¨€ï¼Œcompute_costå‡½æ•°è®¡ç®—äº†é¢„æµ‹å€¼ï¼ˆA2ï¼‰å’ŒçœŸå®å€¼ï¼ˆYï¼‰ä¹‹é—´çš„äº¤å‰ç†µæˆæœ¬ã€‚è¿™ä¸ªæˆæœ¬ç”¨äºè¯„ä¼°ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½ã€‚

**Coding**

        # GRADED FUNCTION: compute_cost

       def compute_cost(A2, Y):
           """
           Computes the cross-entropy cost given in equation (13)
    
           Arguments:
           A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
           Y -- "true" labels vector of shape (1, number of examples)

           Returns:
           cost -- cross-entropy cost given equation (13)
    
           """
    
           m = Y.shape[1] # number of examples

           # Compute the cross-entropy cost
           # (â‰ˆ 2 lines of code)
           # logprobs = ...
           # cost = ...
           # YOUR CODE STARTS HERE
           logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)
           cost = - np.sum(logprobs) / m
    
           # YOUR CODE ENDS HERE
    
           cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. 
                                           # E.g., turns [[17]] into 17 
           
           return cost

**ä»£ç æµ‹è¯•è¾“å‡ºï¼Œç»“æœæ˜¯ä»æµ‹è¯•æ–‡ä»¶ä¸­æ¥çš„**

![20](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/87196a98-196a-4965-801d-9d567c30a30e)

**forward propagateç»“æœå¾—åˆ°costä¹‹åï¼Œå°±éœ€è¦è¿›è¡Œbackward propagateå»è·å¾—W1,b1,W2,b2å’Œgradientå‡†å¤‡å»è¿›è¡Œ gradient descent**


<a name='4-5'></a>
### 4.5 - Implement Backpropagation

Using the cache computed during forward propagation, you can now implement backward propagation.

<a name='ex-6'></a>
### Exercise 6 -  backward_propagation

Implement the function `backward_propagation()`.

**Instructions**:
Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You'll want to use the six equations on the right of this slide, since you are building a vectorized implementation.  
![21](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/b67ab817-cb16-455c-b245-d1653ef49575)


<caption><center><font color='purple'><b>Figure 1</b>: Backpropagation. Use the six equations on the right.</font></center></caption>

<!--
$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})$

$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $

$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}$

$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $

$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $

$\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}$

- Note that $*$ denotes elementwise multiplication.
- The notation you will use is common in deep learning coding:
    - dW1 = $\frac{\partial \mathcal{J} }{ \partial W_1 }$
    - db1 = $\frac{\partial \mathcal{J} }{ \partial b_1 }$
    - dW2 = $\frac{\partial \mathcal{J} }{ \partial W_2 }$
    - db2 = $\frac{\partial \mathcal{J} }{ \partial b_2 }$
    
!-->

- Tips:
    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute 
    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`.


ä¸ºäº†è®¡ç®—dZ1ï¼Œä½ éœ€è¦è®¡ç®—ğ‘”[1]â€²ï¼ˆğ‘[1]ï¼‰ã€‚ç”±äºğ‘”[1](.)æ˜¯tanhæ¿€æ´»å‡½æ•°ï¼Œå¦‚æœğ‘=ğ‘”[1](ğ‘§)ï¼Œé‚£ä¹ˆğ‘”[1]â€²(ğ‘§)=1-ğ‘2 ã€‚æ‰€ä»¥ä½ å¯ä»¥ç”¨ï¼ˆ1-np.power(A1, 2)ï¼‰æ¥è®¡ç®—ğ‘”[1]â€²ï¼ˆğ‘[1]ï¼‰ã€‚

æ¯”è¾ƒé‡è¦çš„å°±æ˜¯ï¼Œåœ¨tanhä½œä¸ºhidden layerçš„æ¿€æ´»å‡½æ•°ï¼Œè€Œsigmoidåšä¸ºoutput layerçš„æ¿€æ´»å‡½æ•°ï¼Œ
åœ¨è¿›è¡Œbackpropagationçš„æ—¶å€™ï¼Œå…ˆè®¡ç®—å‡ºoutput éƒ¨åˆ†çš„dW2, db2,dZ2.
ç„¶åå†ç”¨dZ2çš„ç»“æœç®—å‡ºdZ1,ç”±äºhiddenæ˜¯ç”¨tanh(),æ‰€ä»¥è®¡ç®—æ—¶å€™ï¼Œ dZ1 = W[2]TdZ2 * g[1]'(Z1)
æ ¹æ®æ¨å¯¼ï¼šg[1]'(Z1) = 1 - a^2.
è€Œåœ¨Numpyä¸­çš„np.power(æ•°ç»„ï¼Œæ¬¡æ–¹æ•°)å¯ä»¥å¸®åŠ©æˆ‘ä»¬å¯¹ä¸€ä¸ªæ•°ç»„å†…çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½è¿›è¡Œæ¬¡æ–¹è¿ç®—.

**Code**

       # GRADED FUNCTION: backward_propagation

       def backward_propagation(parameters, cache, X, Y):
           """
           Implement the backward propagation using the instructions above.
    
           Arguments:
           parameters -- python dictionary containing our parameters 
           cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
           X -- input data of shape (2, number of examples)
           Y -- "true" labels vector of shape (1, number of examples)
    
           Returns:
           grads -- python dictionary containing your gradients with respect to different parameters
           """
           m = X.shape[1]
    
           # First, retrieve W1 and W2 from the dictionary "parameters".
           #(â‰ˆ 2 lines of code)
           # W1 = ...
           # W2 = ...
           # YOUR CODE STARTS HERE
           W1 = parameters["W1"]
           W2 = parameters["W2"]
    
           # YOUR CODE ENDS HERE
        
           # Retrieve also A1 and A2 from dictionary "cache".
           #(â‰ˆ 2 lines of code)
           # A1 = ...
           # A2 = ...
           # YOUR CODE STARTS HERE
           A1 = cache["A1"]
           A2 = cache["A2"]
    
           # YOUR CODE ENDS HERE
    
           # Backward propagation: calculate dW1, db1, dW2, db2. 
           #(â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)
           # dZ2 = ...
           # dW2 = ...
           # db2 = ...
           # dZ1 = ...
           # dW1 = ...
           # db1 = ...
           # YOUR CODE STARTS HERE
           dZ2 = A2 - Y
           dW2 = 1/m * np.dot(dZ2, A1.T)
           db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)
           dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
           dW1 = 1/m * np.dot(dZ1, X.T)
           db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)
    
           # YOUR CODE ENDS HERE
    
           grads = {"dW1": dW1,
                    "db1": db1,
                    "dW2": dW2,
                    "db2": db2}
           
           return grads

![22](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/66d78cc7-716e-4fe7-9fd9-78bcd57a5354)

**åœ¨å¾—åˆ°äº†backward propagationä¹‹åå°±å¯ä»¥è¿›è¡Œå‚æ•°æ›´æ–°äº†ï¼Œgradient descent**

<a name='4-6'></a>
### 4.6 - Update Parameters 

<a name='ex-7'></a>
### Exercise 7 - update_parameters

Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).

**General gradient descent rule**: $\theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$ where $\alpha$ is the learning rate and $\theta$ represents a parameter.
![sgd](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/8e182d5c-4264-4883-a50f-4f28c0da1ddb)
![sgd_bad](https://github.com/JoneSu1/Deep-learning-techniques-based-on-python-study-notes-and-project-records/assets/103999272/7b63007a-c389-4500-b9d9-c40e872767ba)


<caption><center><font color='purple'><b>Figure 2</b>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</font></center></caption>

**Hint**

- Use `copy.deepcopy(...)` when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.
